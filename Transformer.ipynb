{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-layer architecture for next token prediction task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports and Utility functions\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import numpy as np\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "import flax.serialization\n",
    "\n",
    "# Define utility functions for attention mechanism\n",
    "def causal_mask(size):\n",
    "    mask = np.tril(np.ones((size, size), dtype=np.bool_), k=0)\n",
    "    return jnp.array(mask)\n",
    "\n",
    "def cross_entropy_loss(logits, labels):\n",
    "    log_probs = jax.nn.log_softmax(logits)\n",
    "    return -jnp.sum(labels * log_probs) / labels.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    embed_dim: int\n",
    "    num_heads: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.head_dim = self.embed_dim \n",
    "        self.qkv = nn.Dense(features=self.embed_dim * 3 * self.num_heads, use_bias=False)\n",
    "        self.out = nn.Dense(features=self.embed_dim)\n",
    "\n",
    "    def __call__(self, x, mask=None):\n",
    "        batch_size, seq_length,_ = x.shape\n",
    "        qkv = self.qkv(x) #qkb are not the params, theyre W_Q, W_K, W_V applied to the batch !\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3, self.head_dim)\n",
    "        qkv = qkv.transpose((2, 0, 1, 3, 4))  # (num_heads, batch_size, seq_length, 3, head_dim)\n",
    "        q, k, v = qkv[:, :, :, 0, :], qkv[:, :, :, 1, :], qkv[:, :, :, 2, :]\n",
    "        attn_weights = jnp.einsum('hbqd,hbkd->hbqk', q, k) / jnp.sqrt(self.head_dim) # einstein summation\n",
    "        #then check if we want to normalize or not / jnp.sqrt(self.head_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            attn_weights = jnp.where(mask[None, None, :, :], attn_weights, -1e10)\n",
    "\n",
    "        attn_weights = jax.nn.softmax(attn_weights, axis=-1) #axis=-1 is the last axis i.e. 'row-wise'\n",
    "        attn_output = jnp.einsum('hbqk,hbvd->hbqd', attn_weights, v) # (num_heads, batch_size, seq_length, head_dim)\n",
    "        attn_output = attn_output.transpose((1, 2, 0, 3))  # (batch_size, seq_length, num_heads, head_dim)\n",
    "        attn_output = attn_output.reshape(batch_size, seq_length, self.num_heads * self.head_dim)\n",
    "        return self.out(attn_output)\n",
    "\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    embed_dim: int\n",
    "    num_heads: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.self_attn = MultiHeadSelfAttention(embed_dim=self.embed_dim, num_heads=self.num_heads)\n",
    "        self.ln = nn.LayerNorm()\n",
    "\n",
    "    def __call__(self, x, mask=None):\n",
    "        attn_output = self.self_attn(x, mask=mask)\n",
    "        x = x + attn_output\n",
    "        x = self.ln(x)\n",
    "        return x\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    #vocab_size: int\n",
    "    layer_dims: list\n",
    "    num_heads: list\n",
    "\n",
    "    def setup(self):\n",
    "        #self.embedding = nn.Embed(num_embeddings=self.vocab_size, features=self.layer_dims[0]) # no embedding!\n",
    "        self.layers = [TransformerDecoderLayer(embed_dim=layer_dim, num_heads=num_heads) \n",
    "                       for layer_dim, num_heads in zip(self.layer_dims, self.num_heads)]\n",
    "        self.ln = nn.LayerNorm()\n",
    "\n",
    "    def __call__(self, x, mask=None):\n",
    "        #x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=mask)\n",
    "        x = self.ln(x)\n",
    "        return x\n",
    "\n",
    "class NextTokenPredictor(nn.Module):\n",
    "    vocab_size: int\n",
    "    layer_dims: list\n",
    "    num_heads: list\n",
    "\n",
    "    def setup(self):\n",
    "        self.decoder = TransformerDecoder(\n",
    "            #vocab_size=self.vocab_size,\n",
    "            layer_dims=self.layer_dims,\n",
    "            num_heads=self.num_heads\n",
    "        )\n",
    "        self.out = nn.Dense(features=self.vocab_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        seq_length = x.shape[1]\n",
    "        mask = causal_mask(seq_length)\n",
    "        decoder_output = self.decoder(x, mask=mask)\n",
    "        logits = self.out(decoder_output)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model definition\n",
    "\n",
    "S = 10  # Cardinality of the alphabet\n",
    "T = 20  # Sequence length\n",
    "m1 = 2  # Heads in the first layer\n",
    "m2 = 1  # Heads in the second layer\n",
    "num_heads = [m1, m2]  # Number of heads for each layer\n",
    "\n",
    "d_0 = S + T # Dimension of the input sequence\n",
    "d_1 = (1 + m1) * d_0  \n",
    "d_2 = (1 + m2) * d_1  # embedding dimension  \n",
    "# definition of d_l in the paper. \n",
    "\n",
    "\n",
    "\n",
    "vocab_size = S\n",
    "layer_dims = [d_2, d_2]  \n",
    "\n",
    "model = NextTokenPredictor(\n",
    "    vocab_size=vocab_size,\n",
    "    layer_dims=layer_dims,\n",
    "    num_heads=num_heads\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation \n",
    "    \n",
    "def create_3gram_transition_matrix(S):\n",
    "    \"\"\"\n",
    "    Create a transition matrix for a 3-gram model by sampling from a Dirichlet prior.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are 2-grams (tuples of integers) and values are the probability distributions\n",
    "          over the next words (integers from 0 to S-1).\n",
    "    \"\"\"\n",
    "    # Create a list of all possible 2-grams from the vocabulary\n",
    "    two_grams = [(i, j) for i in range(S) for j in range(S)]\n",
    "\n",
    "    transition_matrix = {}     # Initialize the transition matrix as a dictionary\n",
    "    alpha = 1.0     # Dirichlet parameter alpha\n",
    " \n",
    "    for two_gram in two_grams:\n",
    "        next_word_probs = np.random.dirichlet([alpha] * S)         # Sample a probability distribution over the next words\n",
    "        transition_matrix[two_gram] = next_word_probs\n",
    "    return transition_matrix\n",
    "\n",
    "\n",
    "def generate_sequence(transition_matrix, T):\n",
    "    \"\"\"\n",
    "    Generate a sequence of length T given a transition matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    transition_matrix (dict): The transition matrix where keys are 2-grams (tuples of integers)\n",
    "                              and values are the probability distributions over the next words.\n",
    "    T (int): Length of the sequence to generate.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of integers representing the generated sequence.\n",
    "    \"\"\"\n",
    "    two_grams = list(transition_matrix.keys())     # Extract the list of 2-grams from the transition matrix\n",
    "    \n",
    "    # Randomly choose an initial 2-gram\n",
    "    current_2gram = two_grams[np.random.choice(len(two_grams))]\n",
    "    \n",
    "    sequence = list(current_2gram)     # Initialize the sequence with the chosen 2-gram\n",
    "    \n",
    "    # Generate the sequence\n",
    "    for _ in range(T - 2):\n",
    "        next_word_probs = transition_matrix[current_2gram]      # Get the next word probability distribution for the current 2-gram\n",
    "        next_word = np.random.choice(range(len(next_word_probs)), p=next_word_probs)         # Sample the next word \n",
    "        sequence.append(next_word)\n",
    "        # Update the current 2-gram\n",
    "        current_2gram = (current_2gram[1], next_word)\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding \n",
    "def embed(sequences):\n",
    "    embedded_sequences = np.zeros((sequences.shape[0], sequences.shape[1], d_2), dtype=np.int32)\n",
    "    for i in range(sequences.shape[0]):\n",
    "        for j in range(sequences.shape[1]):\n",
    "            embedded_sequences[i, j, sequences[i, j]] = 1\n",
    "            embedded_sequences[i, j, S + j] = 1\n",
    "    return embedded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 53.603580474853516\n",
      "Epoch 2, Loss: 47.22572708129883\n",
      "Epoch 3, Loss: 47.35809326171875\n",
      "Epoch 4, Loss: 46.211910247802734\n",
      "Epoch 5, Loss: 45.92648696899414\n",
      "Epoch 6, Loss: 45.73078536987305\n",
      "Epoch 7, Loss: 45.839717864990234\n",
      "Epoch 8, Loss: 45.860652923583984\n",
      "Epoch 9, Loss: 45.68718338012695\n",
      "Epoch 10, Loss: 45.60344696044922\n",
      "Epoch 11, Loss: 45.32526397705078\n",
      "Epoch 12, Loss: 45.17752456665039\n",
      "Epoch 13, Loss: 45.36554718017578\n",
      "Epoch 14, Loss: 45.2417106628418\n",
      "Epoch 15, Loss: 45.23862075805664\n",
      "Epoch 16, Loss: 44.87702941894531\n",
      "Epoch 17, Loss: 44.82986068725586\n",
      "Epoch 18, Loss: 44.633026123046875\n",
      "Epoch 19, Loss: 44.680641174316406\n",
      "Epoch 20, Loss: 44.51638412475586\n",
      "Epoch 21, Loss: 44.45162582397461\n",
      "Epoch 22, Loss: 44.14162826538086\n",
      "Epoch 23, Loss: 44.344242095947266\n",
      "Epoch 24, Loss: 44.448482513427734\n",
      "Epoch 25, Loss: 44.17996597290039\n",
      "Epoch 26, Loss: 44.36270523071289\n",
      "Epoch 27, Loss: 44.37835693359375\n",
      "Epoch 28, Loss: 44.426475524902344\n",
      "Epoch 29, Loss: 44.32596969604492\n",
      "Epoch 30, Loss: 44.241146087646484\n",
      "Epoch 31, Loss: 44.6097526550293\n",
      "Epoch 32, Loss: 44.25020980834961\n",
      "Epoch 33, Loss: 44.47068405151367\n",
      "Epoch 34, Loss: 44.36087417602539\n",
      "Epoch 35, Loss: 44.396881103515625\n",
      "Epoch 36, Loss: 44.3922119140625\n",
      "Epoch 37, Loss: 44.368919372558594\n",
      "Epoch 38, Loss: 44.234657287597656\n",
      "Epoch 39, Loss: 44.27554702758789\n",
      "Epoch 40, Loss: 44.4655647277832\n",
      "Epoch 41, Loss: 44.267879486083984\n",
      "Epoch 42, Loss: 44.31122589111328\n",
      "Epoch 43, Loss: 44.368770599365234\n",
      "Epoch 44, Loss: 44.491031646728516\n",
      "Epoch 45, Loss: 44.29684066772461\n",
      "Epoch 46, Loss: 44.472320556640625\n",
      "Epoch 47, Loss: 44.49107360839844\n",
      "Epoch 48, Loss: 44.37894058227539\n",
      "Epoch 49, Loss: 44.48965835571289\n",
      "Epoch 50, Loss: 44.472232818603516\n",
      "Epoch 51, Loss: 44.47028732299805\n",
      "Epoch 52, Loss: 44.10649108886719\n",
      "Epoch 53, Loss: 44.659629821777344\n",
      "Epoch 54, Loss: 44.39607620239258\n",
      "Epoch 55, Loss: 44.43867874145508\n",
      "Epoch 56, Loss: 44.59490966796875\n",
      "Epoch 57, Loss: 44.4004020690918\n",
      "Epoch 58, Loss: 44.58742141723633\n",
      "Epoch 59, Loss: 44.295082092285156\n",
      "Epoch 60, Loss: 44.32880783081055\n",
      "Epoch 61, Loss: 44.335479736328125\n",
      "Epoch 62, Loss: 44.24909591674805\n",
      "Epoch 63, Loss: 44.42070388793945\n",
      "Epoch 64, Loss: 44.333839416503906\n",
      "Epoch 65, Loss: 44.50643539428711\n",
      "Epoch 66, Loss: 44.621788024902344\n",
      "Epoch 67, Loss: 45.87957000732422\n",
      "Epoch 68, Loss: 45.14954376220703\n",
      "Epoch 69, Loss: 45.179100036621094\n",
      "Epoch 70, Loss: 44.991302490234375\n",
      "Epoch 71, Loss: 44.544185638427734\n",
      "Epoch 72, Loss: 44.506614685058594\n",
      "Epoch 73, Loss: 44.47495651245117\n",
      "Epoch 74, Loss: 44.38112258911133\n",
      "Epoch 75, Loss: 44.29076385498047\n",
      "Epoch 76, Loss: 44.27814865112305\n",
      "Epoch 77, Loss: 44.48957443237305\n",
      "Epoch 78, Loss: 44.09284591674805\n",
      "Epoch 79, Loss: 44.313045501708984\n",
      "Epoch 80, Loss: 44.38086700439453\n",
      "Epoch 81, Loss: 44.45760726928711\n",
      "Epoch 82, Loss: 44.39781951904297\n",
      "Epoch 83, Loss: 44.46950149536133\n",
      "Epoch 84, Loss: 44.53125762939453\n",
      "Epoch 85, Loss: 44.5166015625\n",
      "Epoch 86, Loss: 44.39497375488281\n",
      "Epoch 87, Loss: 44.28343200683594\n",
      "Epoch 88, Loss: 44.326969146728516\n",
      "Epoch 89, Loss: 44.43820571899414\n",
      "Epoch 90, Loss: 44.352699279785156\n",
      "Epoch 91, Loss: 44.3292236328125\n",
      "Epoch 92, Loss: 44.45756149291992\n",
      "Epoch 93, Loss: 44.48122024536133\n",
      "Epoch 94, Loss: 44.531982421875\n",
      "Epoch 95, Loss: 44.35748291015625\n",
      "Epoch 96, Loss: 44.498409271240234\n",
      "Epoch 97, Loss: 44.35244369506836\n",
      "Epoch 98, Loss: 44.2427864074707\n",
      "Epoch 99, Loss: 44.355712890625\n",
      "Epoch 100, Loss: 44.465354919433594\n",
      "Epoch 101, Loss: 44.30923080444336\n",
      "Epoch 102, Loss: 44.29136276245117\n",
      "Epoch 103, Loss: 44.277103424072266\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, logits\n\u001b[1;32m     62\u001b[0m grad_fn \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvalue_and_grad(loss_fn, has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 63\u001b[0m (loss, _), grads \u001b[38;5;241m=\u001b[39m grad_fn(params)\n\u001b[1;32m     64\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Update parameters\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/jax/_src/api.py:693\u001b[0m, in \u001b[0;36mvalue_and_grad.<locals>.value_and_grad_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    691\u001b[0m   ans, vjp_py \u001b[38;5;241m=\u001b[39m _vjp(f_partial, \u001b[38;5;241m*\u001b[39mdyn_args)\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 693\u001b[0m   ans, vjp_py, aux \u001b[38;5;241m=\u001b[39m _vjp(\n\u001b[1;32m    694\u001b[0m       f_partial, \u001b[38;5;241m*\u001b[39mdyn_args, has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    695\u001b[0m _check_scalar(ans)\n\u001b[1;32m    696\u001b[0m tree_map(partial(_check_output_dtype_grad, holomorphic), ans)\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/jax/_src/api.py:2182\u001b[0m, in \u001b[0;36m_vjp\u001b[0;34m(fun, has_aux, *primals)\u001b[0m\n\u001b[1;32m   2180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2181\u001b[0m   flat_fun, out_aux_trees \u001b[38;5;241m=\u001b[39m flatten_fun_nokwargs2(fun, in_tree)\n\u001b[0;32m-> 2182\u001b[0m   out_primal, out_vjp, aux \u001b[38;5;241m=\u001b[39m ad\u001b[38;5;241m.\u001b[39mvjp(\n\u001b[1;32m   2183\u001b[0m       flat_fun, primals_flat, has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2184\u001b[0m   out_tree, aux_tree \u001b[38;5;241m=\u001b[39m out_aux_trees()\n\u001b[1;32m   2185\u001b[0m out_primal_py \u001b[38;5;241m=\u001b[39m tree_unflatten(out_tree, out_primal)\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/jax/_src/interpreters/ad.py:145\u001b[0m, in \u001b[0;36mvjp\u001b[0;34m(traceable, primals, has_aux)\u001b[0m\n\u001b[1;32m    143\u001b[0m   out_primals, pvals, jaxpr, consts \u001b[38;5;241m=\u001b[39m linearize(traceable, \u001b[38;5;241m*\u001b[39mprimals)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m   out_primals, pvals, jaxpr, consts, aux \u001b[38;5;241m=\u001b[39m linearize(traceable, \u001b[38;5;241m*\u001b[39mprimals, has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munbound_vjp\u001b[39m(pvals, jaxpr, consts, \u001b[38;5;241m*\u001b[39mcts):\n\u001b[1;32m    148\u001b[0m   cts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(ct \u001b[38;5;28;01mfor\u001b[39;00m ct, pval \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(cts, pvals) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pval\u001b[38;5;241m.\u001b[39mis_known())\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/jax/_src/interpreters/ad.py:132\u001b[0m, in \u001b[0;36mlinearize\u001b[0;34m(traceable, *primals, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m _, in_tree \u001b[38;5;241m=\u001b[39m tree_flatten(((primals, primals), {}))\n\u001b[1;32m    131\u001b[0m jvpfun_flat, out_tree \u001b[38;5;241m=\u001b[39m flatten_fun(jvpfun, in_tree)\n\u001b[0;32m--> 132\u001b[0m jaxpr, out_pvals, consts \u001b[38;5;241m=\u001b[39m pe\u001b[38;5;241m.\u001b[39mtrace_to_jaxpr_nounits(jvpfun_flat, in_pvals)\n\u001b[1;32m    133\u001b[0m out_primals_pvals, out_tangents_pvals \u001b[38;5;241m=\u001b[39m tree_unflatten(out_tree(), out_pvals)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(out_primal_pval\u001b[38;5;241m.\u001b[39mis_known() \u001b[38;5;28;01mfor\u001b[39;00m out_primal_pval \u001b[38;5;129;01min\u001b[39;00m out_primals_pvals)\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/jax/_src/profiler.py:335\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    336\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/jax/_src/interpreters/partial_eval.py:774\u001b[0m, in \u001b[0;36mtrace_to_jaxpr_nounits\u001b[0;34m(fun, pvals, instantiate)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m core\u001b[38;5;241m.\u001b[39mnew_main(JaxprTrace, name_stack\u001b[38;5;241m=\u001b[39mcurrent_name_stack) \u001b[38;5;28;01mas\u001b[39;00m main:\n\u001b[1;32m    773\u001b[0m   fun \u001b[38;5;241m=\u001b[39m trace_to_subjaxpr_nounits(fun, main, instantiate)\n\u001b[0;32m--> 774\u001b[0m   jaxpr, (out_pvals, consts, env) \u001b[38;5;241m=\u001b[39m fun\u001b[38;5;241m.\u001b[39mcall_wrapped(pvals)\n\u001b[1;32m    775\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env\n\u001b[1;32m    776\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m main, fun, env\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/jax/_src/linear_util.py:192\u001b[0m, in \u001b[0;36mWrappedFun.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m gen \u001b[38;5;241m=\u001b[39m gen_static_args \u001b[38;5;241m=\u001b[39m out_store \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m   ans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    194\u001b[0m   \u001b[38;5;66;03m# Some transformations yield from inside context managers, so we have to\u001b[39;00m\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;66;03m# interrupt them before reraising the exception. Otherwise they will only\u001b[39;00m\n\u001b[1;32m    196\u001b[0m   \u001b[38;5;66;03m# get garbage-collected at some later time, running their cleanup tasks\u001b[39;00m\n\u001b[1;32m    197\u001b[0m   \u001b[38;5;66;03m# only after this exception is handled, which can corrupt the global\u001b[39;00m\n\u001b[1;32m    198\u001b[0m   \u001b[38;5;66;03m# state.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m   \u001b[38;5;28;01mwhile\u001b[39;00m stack:\n",
      "Cell \u001b[0;32mIn[13], line 58\u001b[0m, in \u001b[0;36mloss_fn\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_fn\u001b[39m(params):\n\u001b[0;32m---> 58\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mapply(params, batch_sequences)\n\u001b[1;32m     59\u001b[0m     loss \u001b[38;5;241m=\u001b[39m cross_entropy_loss(logits, batch_targets)\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, logits\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/flax/linen/module.py:2226\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, variables, rngs, method, mutable, capture_intermediates, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2224\u001b[0m   method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m\n\u001b[1;32m   2225\u001b[0m method \u001b[38;5;241m=\u001b[39m _get_unbound_fn(method)\n\u001b[0;32m-> 2226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m apply(\n\u001b[1;32m   2227\u001b[0m   method,\n\u001b[1;32m   2228\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2229\u001b[0m   mutable\u001b[38;5;241m=\u001b[39mmutable,\n\u001b[1;32m   2230\u001b[0m   capture_intermediates\u001b[38;5;241m=\u001b[39mcapture_intermediates,\n\u001b[1;32m   2231\u001b[0m )(variables, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, rngs\u001b[38;5;241m=\u001b[39mrngs)\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/flax/core/scope.py:1101\u001b[0m, in \u001b[0;36mapply.<locals>.wrapper\u001b[0;34m(variables, rngs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mApplyScopeInvalidVariablesStructureError(variables)\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m bind(\n\u001b[1;32m   1099\u001b[0m   variables, rngs\u001b[38;5;241m=\u001b[39mrngs, mutable\u001b[38;5;241m=\u001b[39mmutable, flags\u001b[38;5;241m=\u001b[39mflags\n\u001b[1;32m   1100\u001b[0m )\u001b[38;5;241m.\u001b[39mtemporary() \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[0;32m-> 1101\u001b[0m   y \u001b[38;5;241m=\u001b[39m fn(root, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mutable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m   1103\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m y, root\u001b[38;5;241m.\u001b[39mmutable_variables()\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/flax/linen/module.py:3006\u001b[0m, in \u001b[0;36mapply.<locals>.scope_fn\u001b[0;34m(scope, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3004\u001b[0m _context\u001b[38;5;241m.\u001b[39mcapture_stack\u001b[38;5;241m.\u001b[39mappend(capture_intermediates)\n\u001b[1;32m   3005\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3006\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(module\u001b[38;5;241m.\u001b[39mclone(parent\u001b[38;5;241m=\u001b[39mscope, _deep_clone\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3007\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   3008\u001b[0m   _context\u001b[38;5;241m.\u001b[39mcapture_stack\u001b[38;5;241m.\u001b[39mpop()\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/flax/linen/module.py:701\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], Module):\n\u001b[1;32m    700\u001b[0m   \u001b[38;5;28mself\u001b[39m, args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m], args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 701\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_wrapped_method(fun, args, kwargs)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/flax/linen/module.py:1233\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_named_call:\n\u001b[1;32m   1232\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[38;5;28mself\u001b[39m, fun)):\n\u001b[0;32m-> 1233\u001b[0m     y \u001b[38;5;241m=\u001b[39m run_fun(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1235\u001b[0m   y \u001b[38;5;241m=\u001b[39m run_fun(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[9], line 77\u001b[0m, in \u001b[0;36mNextTokenPredictor.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     75\u001b[0m seq_length \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     76\u001b[0m mask \u001b[38;5;241m=\u001b[39m causal_mask(seq_length)\n\u001b[0;32m---> 77\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x, mask\u001b[38;5;241m=\u001b[39mmask)\n\u001b[1;32m     78\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout(decoder_output)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/flax/linen/module.py:701\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], Module):\n\u001b[1;32m    700\u001b[0m   \u001b[38;5;28mself\u001b[39m, args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m], args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 701\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_wrapped_method(fun, args, kwargs)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/flax/linen/module.py:1233\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_named_call:\n\u001b[1;32m   1232\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[38;5;28mself\u001b[39m, fun)):\n\u001b[0;32m-> 1233\u001b[0m     y \u001b[38;5;241m=\u001b[39m run_fun(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1235\u001b[0m   y \u001b[38;5;241m=\u001b[39m run_fun(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[9], line 57\u001b[0m, in \u001b[0;36mTransformerDecoder.__call__\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m#x = self.embedding(x)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 57\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x, mask\u001b[38;5;241m=\u001b[39mmask)\n\u001b[1;32m     58\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln(x)\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/flax/linen/module.py:701\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], Module):\n\u001b[1;32m    700\u001b[0m   \u001b[38;5;28mself\u001b[39m, args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m], args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 701\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_wrapped_method(fun, args, kwargs)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/flax/linen/module.py:1233\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_named_call:\n\u001b[1;32m   1232\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[38;5;28mself\u001b[39m, fun)):\n\u001b[0;32m-> 1233\u001b[0m     y \u001b[38;5;241m=\u001b[39m run_fun(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1235\u001b[0m   y \u001b[38;5;241m=\u001b[39m run_fun(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[9], line 38\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.__call__\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 38\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(x, mask\u001b[38;5;241m=\u001b[39mmask)\n\u001b[1;32m     39\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m attn_output\n\u001b[1;32m     40\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/flax/linen/module.py:701\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], Module):\n\u001b[1;32m    700\u001b[0m   \u001b[38;5;28mself\u001b[39m, args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m], args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 701\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_wrapped_method(fun, args, kwargs)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/flax/linen/module.py:1233\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_named_call:\n\u001b[1;32m   1232\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[38;5;28mself\u001b[39m, fun)):\n\u001b[0;32m-> 1233\u001b[0m     y \u001b[38;5;241m=\u001b[39m run_fun(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1235\u001b[0m   y \u001b[38;5;241m=\u001b[39m run_fun(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[9], line 26\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.__call__\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     24\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m))  \u001b[38;5;66;03m# (batch_size, seq_length, num_heads, head_dim)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(batch_size, seq_length, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout(attn_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/flax/linen/module.py:701\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], Module):\n\u001b[1;32m    700\u001b[0m   \u001b[38;5;28mself\u001b[39m, args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m], args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 701\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_wrapped_method(fun, args, kwargs)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/flax/linen/module.py:1233\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_named_call:\n\u001b[1;32m   1232\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[38;5;28mself\u001b[39m, fun)):\n\u001b[0;32m-> 1233\u001b[0m     y \u001b[38;5;241m=\u001b[39m run_fun(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1235\u001b[0m   y \u001b[38;5;241m=\u001b[39m run_fun(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/flax/linen/linear.py:268\u001b[0m, in \u001b[0;36mDense.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    267\u001b[0m   bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m inputs, kernel, bias \u001b[38;5;241m=\u001b[39m promote_dtype(inputs, kernel, bias, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdot_general_cls \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m   dot_general \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdot_general_cls()\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/flax/linen/dtypes.py:92\u001b[0m, in \u001b[0;36mpromote_dtype\u001b[0;34m(dtype, inexact, *args)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" \"Promotes input arguments to a specified or inferred dtype.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03mAll args are cast to the same dtype. See ``canonicalize_dtype`` for how\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m  The arguments cast to arrays of the same dtype.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     91\u001b[0m dtype \u001b[38;5;241m=\u001b[39m canonicalize_dtype(\u001b[38;5;241m*\u001b[39margs, dtype\u001b[38;5;241m=\u001b[39mdtype, inexact\u001b[38;5;241m=\u001b[39minexact)\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [jnp\u001b[38;5;241m.\u001b[39masarray(x, dtype) \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m args]\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:2683\u001b[0m, in \u001b[0;36masarray\u001b[0;34m(a, dtype, order, copy)\u001b[0m\n\u001b[1;32m   2681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2682\u001b[0m   dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(dtype, allow_extended_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m-> 2683\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array(a, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(copy), order\u001b[38;5;241m=\u001b[39morder)\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:2608\u001b[0m, in \u001b[0;36marray\u001b[0;34m(object, dtype, copy, order, ndmin)\u001b[0m\n\u001b[1;32m   2605\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2606\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected input type for array: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2608\u001b[0m out_array: Array \u001b[38;5;241m=\u001b[39m lax_internal\u001b[38;5;241m.\u001b[39m_convert_element_type(\n\u001b[1;32m   2609\u001b[0m     out, dtype, weak_type\u001b[38;5;241m=\u001b[39mweak_type)\n\u001b[1;32m   2610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ndmin \u001b[38;5;241m>\u001b[39m ndim(out_array):\n\u001b[1;32m   2611\u001b[0m   out_array \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mexpand_dims(out_array, \u001b[38;5;28mrange\u001b[39m(ndmin \u001b[38;5;241m-\u001b[39m ndim(out_array)))\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/jax/_src/lax/lax.py:559\u001b[0m, in \u001b[0;36m_convert_element_type\u001b[0;34m(operand, new_dtype, weak_type)\u001b[0m\n\u001b[1;32m    557\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m type_cast(Array, operand)\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 559\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m convert_element_type_p\u001b[38;5;241m.\u001b[39mbind(operand, new_dtype\u001b[38;5;241m=\u001b[39mnew_dtype,\n\u001b[1;32m    560\u001b[0m                                      weak_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(weak_type))\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/jax/_src/core.py:387\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    385\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39menable_checks\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    386\u001b[0m           \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Tracer) \u001b[38;5;129;01mor\u001b[39;00m valid_jaxtype(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)), args\n\u001b[0;32m--> 387\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbind_with_trace(find_top_trace(args), args, params)\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/jax/_src/core.py:391\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[1;32m    390\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m pop_level(trace\u001b[38;5;241m.\u001b[39mlevel):\n\u001b[0;32m--> 391\u001b[0m     out \u001b[38;5;241m=\u001b[39m trace\u001b[38;5;241m.\u001b[39mprocess_primitive(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mmap\u001b[39m(trace\u001b[38;5;241m.\u001b[39mfull_raise, args), params)\n\u001b[1;32m    392\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/jax/_src/interpreters/ad.py:314\u001b[0m, in \u001b[0;36mJVPTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    312\u001b[0m   msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDifferentiation rule for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprimitive\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not implemented\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n\u001b[0;32m--> 314\u001b[0m primal_out, tangent_out \u001b[38;5;241m=\u001b[39m jvp(primals_in, tangents_in, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m primitive\u001b[38;5;241m.\u001b[39mmultiple_results:\n\u001b[1;32m    316\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [JVPTracer(\u001b[38;5;28mself\u001b[39m, x, t) \u001b[38;5;28;01mfor\u001b[39;00m x, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(primal_out, tangent_out)]\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/jax/_src/interpreters/ad.py:530\u001b[0m, in \u001b[0;36mstandard_jvp\u001b[0;34m(jvprules, primitive, primals, tangents, **params)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstandard_jvp\u001b[39m(jvprules, primitive, primals, tangents, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m--> 530\u001b[0m   val_out \u001b[38;5;241m=\u001b[39m primitive\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39mprimals, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    531\u001b[0m   tangents_out \u001b[38;5;241m=\u001b[39m [rule(t, \u001b[38;5;241m*\u001b[39mprimals, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams) \u001b[38;5;28;01mfor\u001b[39;00m rule, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(jvprules, tangents)\n\u001b[1;32m    532\u001b[0m                   \u001b[38;5;28;01mif\u001b[39;00m rule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(t) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Zero]\n\u001b[1;32m    533\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m val_out, functools\u001b[38;5;241m.\u001b[39mreduce(add_tangents, tangents_out, Zero\u001b[38;5;241m.\u001b[39mfrom_value(val_out))\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/jax/_src/core.py:387\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    385\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39menable_checks\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    386\u001b[0m           \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Tracer) \u001b[38;5;129;01mor\u001b[39;00m valid_jaxtype(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)), args\n\u001b[0;32m--> 387\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbind_with_trace(find_top_trace(args), args, params)\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/jax/_src/core.py:391\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[1;32m    390\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m pop_level(trace\u001b[38;5;241m.\u001b[39mlevel):\n\u001b[0;32m--> 391\u001b[0m     out \u001b[38;5;241m=\u001b[39m trace\u001b[38;5;241m.\u001b[39mprocess_primitive(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mmap\u001b[39m(trace\u001b[38;5;241m.\u001b[39mfull_raise, args), params)\n\u001b[1;32m    392\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/jax/_src/core.py:879\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    877\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m call_impl_with_key_reuse_checks(primitive, primitive\u001b[38;5;241m.\u001b[39mimpl, \u001b[38;5;241m*\u001b[39mtracers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m primitive\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;241m*\u001b[39mtracers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.12/site-packages/jax/_src/dispatch.py:86\u001b[0m, in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m     84\u001b[0m prev \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m   outs \u001b[38;5;241m=\u001b[39m fun(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m   lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(prev)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Training\n",
    "import os\n",
    "\n",
    "# Directory to save model parameters\n",
    "save_dir = \"saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Lists to store training loss and model parameters\n",
    "training_losses = []\n",
    "model_params_list = []\n",
    "\n",
    "num_epochs = 2048\n",
    "num_batches = 16\n",
    "batch_size = 1024\n",
    "\n",
    "# Define optimizer with cosine decay schedule\n",
    "num_train_steps = num_epochs * num_batches\n",
    "lr_schedule = optax.cosine_decay_schedule(0.3, num_train_steps)\n",
    "optimizer = optax.chain(optax.adam(learning_rate=lr_schedule), optax.clip_by_global_norm(1.0))\n",
    "\n",
    "# Initialize model and optimizer state\n",
    "rng = random.PRNGKey(0)\n",
    "params = model.init(rng, jnp.zeros((1, T, d_2), dtype=jnp.int32))\n",
    "\n",
    "optimizer_state = optimizer.init(params)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    transition_matrix = create_3gram_transition_matrix(S) # 3-gram transition fixed for the epoch\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        sequences = np.array([generate_sequence(transition_matrix, T) for _ in range(batch_size)])\n",
    "\n",
    "\n",
    "      # Generate target sequences by shifting the input sequences\n",
    "        target_sequences = np.zeros_like(sequences)\n",
    "        target_sequences[:, :-1] = sequences[:, 1:]\n",
    "        \n",
    "        # Convert target sequences to one-hot encoding\n",
    "        sequences_onehot = np.zeros((sequences.shape[0], sequences.shape[1], S), dtype=np.int32)\n",
    "        target_sequences_onehot = np.zeros((target_sequences.shape[0], target_sequences.shape[1], S), dtype=np.int32)\n",
    "        for i in range(sequences.shape[0]):\n",
    "            for j in range(sequences.shape[1]):\n",
    "                sequences_onehot[i, j, sequences[i, j]] = 1\n",
    "                target_sequences_onehot[i, j, target_sequences[i, j]] = 1\n",
    "        \n",
    "        # Embed input sequences\n",
    "        embedded_sequences = embed(sequences)\n",
    "        \n",
    "        # Convert embedded sequences and targets to JAX arrays\n",
    "        batch_sequences = jnp.array(embedded_sequences)\n",
    "        batch_targets = jnp.array(target_sequences_onehot)\n",
    "\n",
    "        \n",
    "        # Compute gradients and loss\n",
    "        def loss_fn(params):\n",
    "            logits = model.apply(params, batch_sequences)\n",
    "            loss = cross_entropy_loss(logits, batch_targets)\n",
    "            return loss, logits\n",
    "        \n",
    "        grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "        (loss, _), grads = grad_fn(params)\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        # Update parameters\n",
    "        updates, optimizer_state = optimizer.update(grads, optimizer_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "    \n",
    "    # Calculate average epoch loss\n",
    "    avg_epoch_loss = epoch_loss / num_batches\n",
    "    training_losses.append(avg_epoch_loss)\n",
    "\n",
    "    # Save model parameters\n",
    "    if (epoch + 1) % 250 == 0:\n",
    "        model_params_list.append(params)\n",
    "        model_path = os.path.join(save_dir, f\"model_epoch_{epoch+1}_num_head_{num_heads[d_0]}.params\")\n",
    "\n",
    "        with open(model_path, \"wb\") as f:\n",
    "            f.write(flax.serialization.to_bytes(params))\n",
    "    \n",
    "    # Print epoch loss\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_epoch_loss}\")\n",
    "\n",
    "# Save training losses to file\n",
    "losses_path = os.path.join(save_dir, \"training_losses_num_head_{num_heads[d_0]}.npy\")\n",
    "np.save(losses_path, np.array(training_losses))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

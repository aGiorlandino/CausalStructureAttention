{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-layer architecture for next token prediction task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports and Utility functions\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import numpy as np\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "import flax.serialization\n",
    "\n",
    "# Define utility functions for attention mechanism\n",
    "def causal_mask(size):\n",
    "    mask = np.tril(np.ones((size, size), dtype=np.bool_), k=0)\n",
    "    return jnp.array(mask)\n",
    "\n",
    "def cross_entropy_loss(logits, labels):\n",
    "    log_probs = jax.nn.log_softmax(logits)\n",
    "    return -jnp.sum(labels * log_probs) / labels.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    embed_dim: int\n",
    "    num_heads: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.head_dim = self.embed_dim \n",
    "        self.qkv = nn.Dense(features=self.embed_dim * 3 * self.num_heads, use_bias=False)\n",
    "        self.out = nn.Dense(features=self.embed_dim)\n",
    "\n",
    "    def __call__(self, x, mask=None):\n",
    "        batch_size, seq_length,_ = x.shape\n",
    "        qkv = self.qkv(x) #qkb are not the params, theyre W_Q, W_K, W_V applied to the batch !\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3, self.head_dim)\n",
    "        qkv = qkv.transpose((2, 0, 1, 3, 4))  # (num_heads, batch_size, seq_length, 3, head_dim)\n",
    "        q, k, v = qkv[:, :, :, 0, :], qkv[:, :, :, 1, :], qkv[:, :, :, 2, :]\n",
    "        attn_weights = jnp.einsum('hbqd,hbkd->hbqk', q, k) / jnp.sqrt(self.head_dim) # einstein summation\n",
    "        #then check if we want to normalize or not / jnp.sqrt(self.head_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            attn_weights = jnp.where(mask[None, None, :, :], attn_weights, -1e10)\n",
    "\n",
    "        attn_weights = jax.nn.softmax(attn_weights, axis=-1) #axis=-1 is the last axis i.e. 'row-wise'\n",
    "        attn_output = jnp.einsum('hbqk,hbvd->hbqd', attn_weights, v) # (num_heads, batch_size, seq_length, head_dim)\n",
    "        attn_output = attn_output.transpose((1, 2, 0, 3))  # (batch_size, seq_length, num_heads, head_dim)\n",
    "        attn_output = attn_output.reshape(batch_size, seq_length, self.num_heads * self.head_dim)\n",
    "        return self.out(attn_output)\n",
    "\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    embed_dim: int\n",
    "    num_heads: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.self_attn = MultiHeadSelfAttention(embed_dim=self.embed_dim, num_heads=self.num_heads)\n",
    "        self.ln = nn.LayerNorm()\n",
    "\n",
    "    def __call__(self, x, mask=None):\n",
    "        attn_output = self.self_attn(x, mask=mask)\n",
    "        x = x + attn_output\n",
    "        x = self.ln(x)\n",
    "        return x\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    #vocab_size: int\n",
    "    layer_dims: list\n",
    "    num_heads: list\n",
    "\n",
    "    def setup(self):\n",
    "        #self.embedding = nn.Embed(num_embeddings=self.vocab_size, features=self.layer_dims[0]) # no embedding!\n",
    "        self.layers = [TransformerDecoderLayer(embed_dim=layer_dim, num_heads=num_heads) \n",
    "                       for layer_dim, num_heads in zip(self.layer_dims, self.num_heads)]\n",
    "        self.ln = nn.LayerNorm()\n",
    "\n",
    "    def __call__(self, x, mask=None):\n",
    "        #x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=mask)\n",
    "        x = self.ln(x)\n",
    "        return x\n",
    "\n",
    "class NextTokenPredictor(nn.Module):\n",
    "    vocab_size: int\n",
    "    layer_dims: list\n",
    "    num_heads: list\n",
    "\n",
    "    def setup(self):\n",
    "        self.decoder = TransformerDecoder(\n",
    "            #vocab_size=self.vocab_size,\n",
    "            layer_dims=self.layer_dims,\n",
    "            num_heads=self.num_heads\n",
    "        )\n",
    "        self.out = nn.Dense(features=self.vocab_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        seq_length = x.shape[1]\n",
    "        mask = causal_mask(seq_length)\n",
    "        decoder_output = self.decoder(x, mask=mask)\n",
    "        logits = self.out(decoder_output)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model definition\n",
    "\n",
    "S = 5  # Cardinality of the alphabet\n",
    "T = 25  # Sequence length\n",
    "m1 = 2  # Heads in the first layer\n",
    "m2 = 1  # Heads in the second layer\n",
    "num_heads = [m1, m2]  # Number of heads for each layer\n",
    "\n",
    "d_0 = S + T # Dimension of the input sequence\n",
    "d_1 = (1 + m1) * d_0  \n",
    "d_2 = (1 + m2) * d_1  # embedding dimension  \n",
    "# definition of d_l in the paper. \n",
    "\n",
    "\n",
    "\n",
    "vocab_size = S\n",
    "layer_dims = [d_2, d_2]  \n",
    "\n",
    "model = NextTokenPredictor(\n",
    "    vocab_size=vocab_size,\n",
    "    layer_dims=layer_dims,\n",
    "    num_heads=num_heads\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of loaded sequences: (1000, 25)\n",
      "Shape of embedded sequences: (1000, 25, 180)\n"
     ]
    }
   ],
   "source": [
    "# Data importing (Tri-grams)\n",
    "# Load sequences from file\n",
    "sequences = np.load('sequences.npy')\n",
    "sequences_onehot = np.zeros((sequences.shape[0], sequences.shape[1], S), dtype=np.int32)\n",
    "for i in range(sequences.shape[0]):\n",
    "    for j in range(sequences.shape[1]):\n",
    "        sequences_onehot[i, j, sequences[i, j]] = 1\n",
    "\n",
    "\n",
    "# Check the shape of the loaded sequences\n",
    "print(\"Shape of loaded sequences:\", sequences.shape)\n",
    "\n",
    "# Embedding\n",
    "embedded_sequences = np.zeros((sequences.shape[0], sequences.shape[1], d_2), dtype=np.int32)\n",
    "for i in range(sequences.shape[0]):\n",
    "    for j in range(sequences.shape[1]):\n",
    "        embedded_sequences[i, j, sequences[i, j]] = 1\n",
    "        embedded_sequences[i, j, S + j] = 1\n",
    "print(\"Shape of embedded sequences:\", embedded_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 54.300697326660156\n",
      "Epoch 2, Loss: 41.69598388671875\n",
      "Epoch 3, Loss: 40.70573425292969\n",
      "Epoch 4, Loss: 41.71133041381836\n",
      "Epoch 5, Loss: 41.74773406982422\n",
      "Epoch 6, Loss: 41.29878234863281\n",
      "Epoch 7, Loss: 43.18394470214844\n",
      "Epoch 8, Loss: 42.737892150878906\n",
      "Epoch 9, Loss: 42.795284271240234\n",
      "Epoch 10, Loss: 42.102535247802734\n",
      "Epoch 11, Loss: 43.38384246826172\n",
      "Epoch 12, Loss: 44.13042449951172\n",
      "Epoch 13, Loss: 44.168190002441406\n",
      "Epoch 14, Loss: 42.92168426513672\n",
      "Epoch 15, Loss: 46.09346008300781\n",
      "Epoch 16, Loss: 44.08689880371094\n",
      "Epoch 17, Loss: 43.18350601196289\n",
      "Epoch 18, Loss: 42.32339859008789\n",
      "Epoch 19, Loss: 42.749000549316406\n",
      "Epoch 20, Loss: 41.82263946533203\n",
      "Epoch 21, Loss: 42.590187072753906\n",
      "Epoch 22, Loss: 42.024959564208984\n",
      "Epoch 23, Loss: 42.88422775268555\n",
      "Epoch 24, Loss: 45.50040817260742\n",
      "Epoch 25, Loss: 43.87590789794922\n",
      "Epoch 26, Loss: 44.017269134521484\n",
      "Epoch 27, Loss: 43.09036636352539\n",
      "Epoch 28, Loss: 42.21644973754883\n",
      "Epoch 29, Loss: 41.703495025634766\n",
      "Epoch 30, Loss: 41.08309555053711\n",
      "Epoch 31, Loss: 41.183773040771484\n",
      "Epoch 32, Loss: 41.71914291381836\n",
      "Epoch 33, Loss: 47.11094665527344\n",
      "Epoch 34, Loss: 48.50517272949219\n",
      "Epoch 35, Loss: 44.92170715332031\n",
      "Epoch 36, Loss: 46.126548767089844\n",
      "Epoch 37, Loss: 42.22177505493164\n",
      "Epoch 38, Loss: 41.87132263183594\n",
      "Epoch 39, Loss: 42.57651901245117\n",
      "Epoch 40, Loss: 43.263790130615234\n",
      "Epoch 41, Loss: 41.567955017089844\n",
      "Epoch 42, Loss: 41.53721237182617\n",
      "Epoch 43, Loss: 42.228004455566406\n",
      "Epoch 44, Loss: 41.5702018737793\n",
      "Epoch 45, Loss: 40.902259826660156\n",
      "Epoch 46, Loss: 42.2280158996582\n",
      "Epoch 47, Loss: 43.63946533203125\n",
      "Epoch 48, Loss: 42.811973571777344\n",
      "Epoch 49, Loss: 42.362144470214844\n",
      "Epoch 50, Loss: 43.0995979309082\n",
      "Epoch 51, Loss: 42.04338836669922\n",
      "Epoch 52, Loss: 42.33545684814453\n",
      "Epoch 53, Loss: 41.8425178527832\n",
      "Epoch 54, Loss: 43.50412368774414\n",
      "Epoch 55, Loss: 44.27305221557617\n",
      "Epoch 56, Loss: 42.943115234375\n",
      "Epoch 57, Loss: 41.76607131958008\n",
      "Epoch 58, Loss: 41.9963264465332\n",
      "Epoch 59, Loss: 40.97646713256836\n",
      "Epoch 60, Loss: 40.66259002685547\n",
      "Epoch 61, Loss: 41.072776794433594\n",
      "Epoch 62, Loss: 42.853206634521484\n",
      "Epoch 63, Loss: 42.470001220703125\n",
      "Epoch 64, Loss: 41.61817169189453\n",
      "Epoch 65, Loss: 41.88678741455078\n",
      "Epoch 66, Loss: 42.38535690307617\n",
      "Epoch 67, Loss: 43.05561065673828\n",
      "Epoch 68, Loss: 42.30213165283203\n",
      "Epoch 69, Loss: 41.69314193725586\n",
      "Epoch 70, Loss: 40.914180755615234\n",
      "Epoch 71, Loss: 41.31325149536133\n",
      "Epoch 72, Loss: 42.28842544555664\n",
      "Epoch 73, Loss: 42.06076431274414\n",
      "Epoch 74, Loss: 41.63141632080078\n",
      "Epoch 75, Loss: 44.76374816894531\n",
      "Epoch 76, Loss: 42.51179122924805\n",
      "Epoch 77, Loss: 40.9467658996582\n",
      "Epoch 78, Loss: 44.050376892089844\n",
      "Epoch 79, Loss: 45.0268440246582\n",
      "Epoch 80, Loss: 45.500675201416016\n",
      "Epoch 81, Loss: 43.345947265625\n",
      "Epoch 82, Loss: 42.943336486816406\n",
      "Epoch 83, Loss: 42.41927719116211\n",
      "Epoch 84, Loss: 42.421836853027344\n",
      "Epoch 85, Loss: 42.14056396484375\n",
      "Epoch 86, Loss: 43.0614128112793\n",
      "Epoch 87, Loss: 42.59682083129883\n",
      "Epoch 88, Loss: 42.40410232543945\n",
      "Epoch 89, Loss: 41.961246490478516\n",
      "Epoch 90, Loss: 41.395423889160156\n",
      "Epoch 91, Loss: 43.329410552978516\n",
      "Epoch 92, Loss: 42.38756561279297\n",
      "Epoch 93, Loss: 42.26764678955078\n",
      "Epoch 94, Loss: 41.32522201538086\n",
      "Epoch 95, Loss: 41.372596740722656\n",
      "Epoch 96, Loss: 41.7922477722168\n",
      "Epoch 97, Loss: 41.94028091430664\n",
      "Epoch 98, Loss: 42.158687591552734\n",
      "Epoch 99, Loss: 42.822593688964844\n",
      "Epoch 100, Loss: 43.11859893798828\n",
      "Epoch 101, Loss: 42.25945281982422\n",
      "Epoch 102, Loss: 42.86922073364258\n",
      "Epoch 103, Loss: 42.63367462158203\n",
      "Epoch 104, Loss: 44.544677734375\n",
      "Epoch 105, Loss: 41.777442932128906\n",
      "Epoch 106, Loss: 41.303802490234375\n",
      "Epoch 107, Loss: 41.612266540527344\n",
      "Epoch 108, Loss: 43.55501937866211\n",
      "Epoch 109, Loss: 47.616455078125\n",
      "Epoch 110, Loss: 43.971824645996094\n",
      "Epoch 111, Loss: 40.981101989746094\n",
      "Epoch 112, Loss: 40.625064849853516\n",
      "Epoch 113, Loss: 41.31230926513672\n",
      "Epoch 114, Loss: 41.32284164428711\n",
      "Epoch 115, Loss: 41.447818756103516\n",
      "Epoch 116, Loss: 42.367713928222656\n",
      "Epoch 117, Loss: 43.278961181640625\n",
      "Epoch 118, Loss: 41.733978271484375\n",
      "Epoch 119, Loss: 41.28458023071289\n",
      "Epoch 120, Loss: 41.499210357666016\n",
      "Epoch 121, Loss: 42.5544319152832\n",
      "Epoch 122, Loss: 43.76836013793945\n",
      "Epoch 123, Loss: 42.92404556274414\n",
      "Epoch 124, Loss: 42.8250617980957\n",
      "Epoch 125, Loss: 42.071632385253906\n",
      "Epoch 126, Loss: 42.72792053222656\n",
      "Epoch 127, Loss: 41.932220458984375\n",
      "Epoch 128, Loss: 41.95792770385742\n",
      "Epoch 129, Loss: 42.4101448059082\n",
      "Epoch 130, Loss: 43.97403335571289\n",
      "Epoch 131, Loss: 42.88325881958008\n",
      "Epoch 132, Loss: 41.85254669189453\n",
      "Epoch 133, Loss: 41.67337417602539\n",
      "Epoch 134, Loss: 42.56305694580078\n",
      "Epoch 135, Loss: 42.82107162475586\n",
      "Epoch 136, Loss: 42.098453521728516\n",
      "Epoch 137, Loss: 41.4567756652832\n",
      "Epoch 138, Loss: 41.22705078125\n",
      "Epoch 139, Loss: 41.318763732910156\n",
      "Epoch 140, Loss: 40.917667388916016\n",
      "Epoch 141, Loss: 41.3757438659668\n",
      "Epoch 142, Loss: 42.41458511352539\n",
      "Epoch 143, Loss: 42.61962127685547\n",
      "Epoch 144, Loss: 41.48830032348633\n",
      "Epoch 145, Loss: 41.624942779541016\n",
      "Epoch 146, Loss: 42.84647750854492\n",
      "Epoch 147, Loss: 42.775814056396484\n",
      "Epoch 148, Loss: 42.25541305541992\n",
      "Epoch 149, Loss: 42.758384704589844\n",
      "Epoch 150, Loss: 41.975624084472656\n",
      "Epoch 151, Loss: 42.4141960144043\n",
      "Epoch 152, Loss: 41.539276123046875\n",
      "Epoch 153, Loss: 42.07475280761719\n",
      "Epoch 154, Loss: 42.20871353149414\n",
      "Epoch 155, Loss: 43.196903228759766\n",
      "Epoch 156, Loss: 41.347373962402344\n",
      "Epoch 157, Loss: 41.58451461791992\n",
      "Epoch 158, Loss: 41.79000473022461\n",
      "Epoch 159, Loss: 40.91846466064453\n",
      "Epoch 160, Loss: 42.37196731567383\n",
      "Epoch 161, Loss: 44.4974479675293\n",
      "Epoch 162, Loss: 47.41012954711914\n",
      "Epoch 163, Loss: 43.42011642456055\n",
      "Epoch 164, Loss: 43.36174011230469\n",
      "Epoch 165, Loss: 41.795963287353516\n",
      "Epoch 166, Loss: 41.0283203125\n",
      "Epoch 167, Loss: 41.649906158447266\n",
      "Epoch 168, Loss: 41.20067596435547\n",
      "Epoch 169, Loss: 41.473533630371094\n",
      "Epoch 170, Loss: 41.13139343261719\n",
      "Epoch 171, Loss: 41.283592224121094\n",
      "Epoch 172, Loss: 41.287864685058594\n",
      "Epoch 173, Loss: 41.24726104736328\n",
      "Epoch 174, Loss: 41.39006042480469\n",
      "Epoch 175, Loss: 41.690208435058594\n",
      "Epoch 176, Loss: 41.24152374267578\n",
      "Epoch 177, Loss: 40.98561477661133\n",
      "Epoch 178, Loss: 41.88311767578125\n",
      "Epoch 179, Loss: 41.557865142822266\n",
      "Epoch 180, Loss: 42.42014694213867\n",
      "Epoch 181, Loss: 41.74736022949219\n",
      "Epoch 182, Loss: 41.28754806518555\n",
      "Epoch 183, Loss: 41.603721618652344\n",
      "Epoch 184, Loss: 43.643943786621094\n",
      "Epoch 185, Loss: 43.700111389160156\n",
      "Epoch 186, Loss: 43.91255569458008\n",
      "Epoch 187, Loss: 41.613243103027344\n",
      "Epoch 188, Loss: 41.42876434326172\n",
      "Epoch 189, Loss: 41.38700485229492\n",
      "Epoch 190, Loss: 42.150672912597656\n",
      "Epoch 191, Loss: 41.607826232910156\n",
      "Epoch 192, Loss: 41.511234283447266\n",
      "Epoch 193, Loss: 41.14043426513672\n",
      "Epoch 194, Loss: 41.52037048339844\n",
      "Epoch 195, Loss: 41.252952575683594\n",
      "Epoch 196, Loss: 42.433311462402344\n",
      "Epoch 197, Loss: 42.20186233520508\n",
      "Epoch 198, Loss: 41.68450164794922\n",
      "Epoch 199, Loss: 41.283199310302734\n",
      "Epoch 200, Loss: 41.54290008544922\n",
      "Epoch 201, Loss: 41.83088302612305\n",
      "Epoch 202, Loss: 41.263153076171875\n",
      "Epoch 203, Loss: 41.09769821166992\n",
      "Epoch 204, Loss: 40.84941482543945\n",
      "Epoch 205, Loss: 41.144432067871094\n",
      "Epoch 206, Loss: 41.3402214050293\n",
      "Epoch 207, Loss: 43.397254943847656\n",
      "Epoch 208, Loss: 41.9459114074707\n",
      "Epoch 209, Loss: 41.537872314453125\n",
      "Epoch 210, Loss: 41.328514099121094\n",
      "Epoch 211, Loss: 40.943302154541016\n",
      "Epoch 212, Loss: 40.90315246582031\n",
      "Epoch 213, Loss: 41.02693176269531\n",
      "Epoch 214, Loss: 40.94776916503906\n",
      "Epoch 215, Loss: 40.96393966674805\n",
      "Epoch 216, Loss: 41.384700775146484\n",
      "Epoch 217, Loss: 41.40892791748047\n",
      "Epoch 218, Loss: 40.7785758972168\n",
      "Epoch 219, Loss: 41.0861701965332\n",
      "Epoch 220, Loss: 41.25210952758789\n",
      "Epoch 221, Loss: 40.90689468383789\n",
      "Epoch 222, Loss: 41.72901153564453\n",
      "Epoch 223, Loss: 41.24857711791992\n",
      "Epoch 224, Loss: 41.16658401489258\n",
      "Epoch 225, Loss: 41.39710235595703\n",
      "Epoch 226, Loss: 42.349464416503906\n",
      "Epoch 227, Loss: 41.7281494140625\n",
      "Epoch 228, Loss: 42.26951599121094\n",
      "Epoch 229, Loss: 41.85930252075195\n",
      "Epoch 230, Loss: 41.462520599365234\n",
      "Epoch 231, Loss: 40.61251449584961\n",
      "Epoch 232, Loss: 40.420738220214844\n",
      "Epoch 233, Loss: 41.005985260009766\n",
      "Epoch 234, Loss: 40.755165100097656\n",
      "Epoch 235, Loss: 40.743438720703125\n",
      "Epoch 236, Loss: 40.773231506347656\n",
      "Epoch 237, Loss: 41.311092376708984\n",
      "Epoch 238, Loss: 43.282344818115234\n",
      "Epoch 239, Loss: 42.22904968261719\n",
      "Epoch 240, Loss: 41.503170013427734\n",
      "Epoch 241, Loss: 41.333343505859375\n",
      "Epoch 242, Loss: 41.616058349609375\n",
      "Epoch 243, Loss: 40.6394157409668\n",
      "Epoch 244, Loss: 40.6400032043457\n",
      "Epoch 245, Loss: 40.67176055908203\n",
      "Epoch 246, Loss: 40.83269500732422\n",
      "Epoch 247, Loss: 40.94866180419922\n",
      "Epoch 248, Loss: 41.377685546875\n",
      "Epoch 249, Loss: 41.60350799560547\n",
      "Epoch 250, Loss: 41.754180908203125\n",
      "Epoch 251, Loss: 41.49394607543945\n",
      "Epoch 252, Loss: 41.79900360107422\n",
      "Epoch 253, Loss: 41.560157775878906\n",
      "Epoch 254, Loss: 41.3121452331543\n",
      "Epoch 255, Loss: 42.66193771362305\n",
      "Epoch 256, Loss: 41.62220764160156\n",
      "Epoch 257, Loss: 41.37495803833008\n",
      "Epoch 258, Loss: 41.80693817138672\n",
      "Epoch 259, Loss: 41.609344482421875\n",
      "Epoch 260, Loss: 41.13654327392578\n",
      "Epoch 261, Loss: 41.58712387084961\n",
      "Epoch 262, Loss: 40.87419891357422\n",
      "Epoch 263, Loss: 41.743011474609375\n",
      "Epoch 264, Loss: 40.654720306396484\n",
      "Epoch 265, Loss: 40.573551177978516\n",
      "Epoch 266, Loss: 41.127899169921875\n",
      "Epoch 267, Loss: 41.54692459106445\n",
      "Epoch 268, Loss: 40.87069320678711\n",
      "Epoch 269, Loss: 41.72116470336914\n",
      "Epoch 270, Loss: 41.75502014160156\n",
      "Epoch 271, Loss: 42.291629791259766\n",
      "Epoch 272, Loss: 43.147117614746094\n",
      "Epoch 273, Loss: 41.488525390625\n",
      "Epoch 274, Loss: 41.35280227661133\n",
      "Epoch 275, Loss: 40.76371383666992\n",
      "Epoch 276, Loss: 40.556209564208984\n",
      "Epoch 277, Loss: 40.49169921875\n",
      "Epoch 278, Loss: 40.943477630615234\n",
      "Epoch 279, Loss: 40.767608642578125\n",
      "Epoch 280, Loss: 41.07830047607422\n",
      "Epoch 281, Loss: 40.91126251220703\n",
      "Epoch 282, Loss: 41.442909240722656\n",
      "Epoch 283, Loss: 42.042991638183594\n",
      "Epoch 284, Loss: 42.51608657836914\n",
      "Epoch 285, Loss: 41.6583137512207\n",
      "Epoch 286, Loss: 41.33305358886719\n",
      "Epoch 287, Loss: 41.7081298828125\n",
      "Epoch 288, Loss: 41.178348541259766\n",
      "Epoch 289, Loss: 41.306121826171875\n",
      "Epoch 290, Loss: 41.28348159790039\n",
      "Epoch 291, Loss: 41.598289489746094\n",
      "Epoch 292, Loss: 41.49085998535156\n",
      "Epoch 293, Loss: 41.56343078613281\n",
      "Epoch 294, Loss: 41.735511779785156\n",
      "Epoch 295, Loss: 41.33198928833008\n",
      "Epoch 296, Loss: 41.24209976196289\n",
      "Epoch 297, Loss: 40.856014251708984\n",
      "Epoch 298, Loss: 41.27996826171875\n",
      "Epoch 299, Loss: 40.935646057128906\n",
      "Epoch 300, Loss: 40.34865188598633\n",
      "Epoch 301, Loss: 40.7862548828125\n",
      "Epoch 302, Loss: 40.7419548034668\n",
      "Epoch 303, Loss: 42.08427810668945\n",
      "Epoch 304, Loss: 40.97138214111328\n",
      "Epoch 305, Loss: 41.845027923583984\n",
      "Epoch 306, Loss: 41.08453369140625\n",
      "Epoch 307, Loss: 40.89680099487305\n",
      "Epoch 308, Loss: 40.75141525268555\n",
      "Epoch 309, Loss: 40.86775588989258\n",
      "Epoch 310, Loss: 41.02836608886719\n",
      "Epoch 311, Loss: 41.3322639465332\n",
      "Epoch 312, Loss: 41.48235321044922\n",
      "Epoch 313, Loss: 40.40099334716797\n",
      "Epoch 314, Loss: 40.36992263793945\n",
      "Epoch 315, Loss: 40.89171600341797\n",
      "Epoch 316, Loss: 40.71885681152344\n",
      "Epoch 317, Loss: 41.03538131713867\n",
      "Epoch 318, Loss: 41.117950439453125\n",
      "Epoch 319, Loss: 41.1706428527832\n",
      "Epoch 320, Loss: 41.17086410522461\n",
      "Epoch 321, Loss: 40.9036865234375\n",
      "Epoch 322, Loss: 40.64567947387695\n",
      "Epoch 323, Loss: 40.424678802490234\n",
      "Epoch 324, Loss: 40.733604431152344\n",
      "Epoch 325, Loss: 40.70458221435547\n",
      "Epoch 326, Loss: 40.799888610839844\n",
      "Epoch 327, Loss: 45.06513595581055\n",
      "Epoch 328, Loss: 45.35184097290039\n",
      "Epoch 329, Loss: 42.03319549560547\n",
      "Epoch 330, Loss: 40.55104064941406\n",
      "Epoch 331, Loss: 40.45404052734375\n",
      "Epoch 332, Loss: 40.68020248413086\n",
      "Epoch 333, Loss: 40.608089447021484\n",
      "Epoch 334, Loss: 40.389747619628906\n",
      "Epoch 335, Loss: 40.61930847167969\n",
      "Epoch 336, Loss: 40.826385498046875\n",
      "Epoch 337, Loss: 41.141029357910156\n",
      "Epoch 338, Loss: 41.1565055847168\n",
      "Epoch 339, Loss: 41.53227996826172\n",
      "Epoch 340, Loss: 41.21140670776367\n",
      "Epoch 341, Loss: 41.551395416259766\n",
      "Epoch 342, Loss: 41.239315032958984\n",
      "Epoch 343, Loss: 41.15948486328125\n",
      "Epoch 344, Loss: 41.11914825439453\n",
      "Epoch 345, Loss: 40.89149856567383\n",
      "Epoch 346, Loss: 41.03215026855469\n",
      "Epoch 347, Loss: 40.75355529785156\n",
      "Epoch 348, Loss: 40.62567138671875\n",
      "Epoch 349, Loss: 41.10564422607422\n",
      "Epoch 350, Loss: 42.21015548706055\n",
      "Epoch 351, Loss: 40.91487121582031\n",
      "Epoch 352, Loss: 40.4744873046875\n",
      "Epoch 353, Loss: 40.61342239379883\n",
      "Epoch 354, Loss: 40.56076431274414\n",
      "Epoch 355, Loss: 40.634254455566406\n",
      "Epoch 356, Loss: 40.68549728393555\n",
      "Epoch 357, Loss: 40.546791076660156\n",
      "Epoch 358, Loss: 40.62258529663086\n",
      "Epoch 359, Loss: 40.350467681884766\n",
      "Epoch 360, Loss: 40.329345703125\n",
      "Epoch 361, Loss: 40.55647277832031\n",
      "Epoch 362, Loss: 40.8007698059082\n",
      "Epoch 363, Loss: 40.627708435058594\n",
      "Epoch 364, Loss: 40.65265655517578\n",
      "Epoch 365, Loss: 40.61220932006836\n",
      "Epoch 366, Loss: 41.124046325683594\n",
      "Epoch 367, Loss: 41.025352478027344\n",
      "Epoch 368, Loss: 41.51750564575195\n",
      "Epoch 369, Loss: 41.195457458496094\n",
      "Epoch 370, Loss: 40.81637954711914\n",
      "Epoch 371, Loss: 40.88887405395508\n",
      "Epoch 372, Loss: 40.58125686645508\n",
      "Epoch 373, Loss: 40.533226013183594\n",
      "Epoch 374, Loss: 40.42298889160156\n",
      "Epoch 375, Loss: 40.34890365600586\n",
      "Epoch 376, Loss: 40.5242919921875\n",
      "Epoch 377, Loss: 40.684783935546875\n",
      "Epoch 378, Loss: 41.781620025634766\n",
      "Epoch 379, Loss: 43.3189811706543\n",
      "Epoch 380, Loss: 41.30128860473633\n",
      "Epoch 381, Loss: 40.602664947509766\n",
      "Epoch 382, Loss: 40.60023498535156\n",
      "Epoch 383, Loss: 40.711151123046875\n",
      "Epoch 384, Loss: 40.7735710144043\n",
      "Epoch 385, Loss: 41.07230758666992\n",
      "Epoch 386, Loss: 41.60585021972656\n",
      "Epoch 387, Loss: 43.005619049072266\n",
      "Epoch 388, Loss: 41.0101203918457\n",
      "Epoch 389, Loss: 41.29625701904297\n",
      "Epoch 390, Loss: 40.93074417114258\n",
      "Epoch 391, Loss: 40.84496307373047\n",
      "Epoch 392, Loss: 41.35430145263672\n",
      "Epoch 393, Loss: 41.186668395996094\n",
      "Epoch 394, Loss: 41.12067413330078\n",
      "Epoch 395, Loss: 40.70216369628906\n",
      "Epoch 396, Loss: 41.62074279785156\n",
      "Epoch 397, Loss: 41.04580307006836\n",
      "Epoch 398, Loss: 40.753658294677734\n",
      "Epoch 399, Loss: 41.848140716552734\n",
      "Epoch 400, Loss: 41.55821990966797\n",
      "Epoch 401, Loss: 41.59904861450195\n",
      "Epoch 402, Loss: 40.692054748535156\n",
      "Epoch 403, Loss: 41.028560638427734\n",
      "Epoch 404, Loss: 43.378700256347656\n",
      "Epoch 405, Loss: 41.63728332519531\n",
      "Epoch 406, Loss: 41.61193084716797\n",
      "Epoch 407, Loss: 40.90826416015625\n",
      "Epoch 408, Loss: 40.97407913208008\n",
      "Epoch 409, Loss: 40.74150085449219\n",
      "Epoch 410, Loss: 40.6418571472168\n",
      "Epoch 411, Loss: 40.8522834777832\n",
      "Epoch 412, Loss: 40.722923278808594\n",
      "Epoch 413, Loss: 40.993621826171875\n",
      "Epoch 414, Loss: 41.519561767578125\n",
      "Epoch 415, Loss: 41.59953689575195\n",
      "Epoch 416, Loss: 40.96847915649414\n",
      "Epoch 417, Loss: 40.55953598022461\n",
      "Epoch 418, Loss: 40.66764831542969\n",
      "Epoch 419, Loss: 41.05130386352539\n",
      "Epoch 420, Loss: 41.59901809692383\n",
      "Epoch 421, Loss: 41.279178619384766\n",
      "Epoch 422, Loss: 41.26947021484375\n",
      "Epoch 423, Loss: 40.28813171386719\n",
      "Epoch 424, Loss: 40.41285705566406\n",
      "Epoch 425, Loss: 40.598876953125\n",
      "Epoch 426, Loss: 40.66414260864258\n",
      "Epoch 427, Loss: 42.24197006225586\n",
      "Epoch 428, Loss: 42.03734588623047\n",
      "Epoch 429, Loss: 41.150657653808594\n",
      "Epoch 430, Loss: 40.8045654296875\n",
      "Epoch 431, Loss: 40.937217712402344\n",
      "Epoch 432, Loss: 40.36079788208008\n",
      "Epoch 433, Loss: 40.52740478515625\n",
      "Epoch 434, Loss: 40.57968521118164\n",
      "Epoch 435, Loss: 40.47499084472656\n",
      "Epoch 436, Loss: 40.60191345214844\n",
      "Epoch 437, Loss: 40.684627532958984\n",
      "Epoch 438, Loss: 40.7203254699707\n",
      "Epoch 439, Loss: 43.86590576171875\n",
      "Epoch 440, Loss: 41.626155853271484\n",
      "Epoch 441, Loss: 41.2437744140625\n",
      "Epoch 442, Loss: 40.80747604370117\n",
      "Epoch 443, Loss: 40.83855438232422\n",
      "Epoch 444, Loss: 40.80248260498047\n",
      "Epoch 445, Loss: 40.8569221496582\n",
      "Epoch 446, Loss: 40.6788215637207\n",
      "Epoch 447, Loss: 40.89271926879883\n",
      "Epoch 448, Loss: 40.73899459838867\n",
      "Epoch 449, Loss: 40.742496490478516\n",
      "Epoch 450, Loss: 40.54395294189453\n",
      "Epoch 451, Loss: 40.831478118896484\n",
      "Epoch 452, Loss: 41.85953140258789\n",
      "Epoch 453, Loss: 40.98709487915039\n",
      "Epoch 454, Loss: 40.622440338134766\n",
      "Epoch 455, Loss: 40.57048797607422\n",
      "Epoch 456, Loss: 40.88119125366211\n",
      "Epoch 457, Loss: 41.1236457824707\n",
      "Epoch 458, Loss: 40.862667083740234\n",
      "Epoch 459, Loss: 41.25041580200195\n",
      "Epoch 460, Loss: 40.65163040161133\n",
      "Epoch 461, Loss: 40.71834945678711\n",
      "Epoch 462, Loss: 40.97059631347656\n",
      "Epoch 463, Loss: 42.027809143066406\n",
      "Epoch 464, Loss: 41.60702133178711\n",
      "Epoch 465, Loss: 40.84027862548828\n",
      "Epoch 466, Loss: 40.54001235961914\n",
      "Epoch 467, Loss: 40.79572296142578\n",
      "Epoch 468, Loss: 40.95138931274414\n",
      "Epoch 469, Loss: 41.445552825927734\n",
      "Epoch 470, Loss: 42.03647232055664\n",
      "Epoch 471, Loss: 41.508182525634766\n",
      "Epoch 472, Loss: 41.84989929199219\n",
      "Epoch 473, Loss: 42.821327209472656\n",
      "Epoch 474, Loss: 42.28781509399414\n",
      "Epoch 475, Loss: 40.80624008178711\n",
      "Epoch 476, Loss: 40.93977737426758\n",
      "Epoch 477, Loss: 40.9149169921875\n",
      "Epoch 478, Loss: 41.42588806152344\n",
      "Epoch 479, Loss: 41.172584533691406\n",
      "Epoch 480, Loss: 40.74006652832031\n",
      "Epoch 481, Loss: 40.60905838012695\n",
      "Epoch 482, Loss: 40.60593795776367\n",
      "Epoch 483, Loss: 40.318092346191406\n",
      "Epoch 484, Loss: 40.47220993041992\n",
      "Epoch 485, Loss: 40.69608688354492\n",
      "Epoch 486, Loss: 40.53754425048828\n",
      "Epoch 487, Loss: 40.26085662841797\n",
      "Epoch 488, Loss: 40.319244384765625\n",
      "Epoch 489, Loss: 40.15569305419922\n",
      "Epoch 490, Loss: 40.18547058105469\n",
      "Epoch 491, Loss: 40.20392608642578\n",
      "Epoch 492, Loss: 40.411827087402344\n",
      "Epoch 493, Loss: 40.34194564819336\n",
      "Epoch 494, Loss: 40.57676696777344\n",
      "Epoch 495, Loss: 40.48394012451172\n",
      "Epoch 496, Loss: 40.52893829345703\n",
      "Epoch 497, Loss: 40.583702087402344\n",
      "Epoch 498, Loss: 40.68739318847656\n",
      "Epoch 499, Loss: 40.843326568603516\n",
      "Epoch 500, Loss: 40.86907958984375\n",
      "Epoch 501, Loss: 40.78082275390625\n",
      "Epoch 502, Loss: 40.37449264526367\n",
      "Epoch 503, Loss: 40.667518615722656\n",
      "Epoch 504, Loss: 40.86539077758789\n",
      "Epoch 505, Loss: 41.16156005859375\n",
      "Epoch 506, Loss: 40.88042449951172\n",
      "Epoch 507, Loss: 40.674598693847656\n",
      "Epoch 508, Loss: 40.69892120361328\n",
      "Epoch 509, Loss: 41.28498077392578\n",
      "Epoch 510, Loss: 41.05896759033203\n",
      "Epoch 511, Loss: 41.24284362792969\n",
      "Epoch 512, Loss: 40.7087287902832\n",
      "Epoch 513, Loss: 40.382423400878906\n",
      "Epoch 514, Loss: 40.4991455078125\n",
      "Epoch 515, Loss: 40.72776794433594\n",
      "Epoch 516, Loss: 40.663658142089844\n",
      "Epoch 517, Loss: 40.24819564819336\n",
      "Epoch 518, Loss: 40.31576156616211\n",
      "Epoch 519, Loss: 40.906063079833984\n",
      "Epoch 520, Loss: 40.71592330932617\n",
      "Epoch 521, Loss: 40.82594680786133\n",
      "Epoch 522, Loss: 41.19554138183594\n",
      "Epoch 523, Loss: 41.80644989013672\n",
      "Epoch 524, Loss: 41.821659088134766\n",
      "Epoch 525, Loss: 43.601078033447266\n",
      "Epoch 526, Loss: 41.08628845214844\n",
      "Epoch 527, Loss: 40.7801399230957\n",
      "Epoch 528, Loss: 40.51174545288086\n",
      "Epoch 529, Loss: 40.49306869506836\n",
      "Epoch 530, Loss: 40.8268928527832\n",
      "Epoch 531, Loss: 41.03657531738281\n",
      "Epoch 532, Loss: 41.14460754394531\n",
      "Epoch 533, Loss: 40.975379943847656\n",
      "Epoch 534, Loss: 40.50288009643555\n",
      "Epoch 535, Loss: 40.47901153564453\n",
      "Epoch 536, Loss: 40.6311149597168\n",
      "Epoch 537, Loss: 40.57853317260742\n",
      "Epoch 538, Loss: 40.69061279296875\n",
      "Epoch 539, Loss: 40.54665756225586\n",
      "Epoch 540, Loss: 40.480567932128906\n",
      "Epoch 541, Loss: 40.27510070800781\n",
      "Epoch 542, Loss: 40.215354919433594\n",
      "Epoch 543, Loss: 40.25593185424805\n",
      "Epoch 544, Loss: 40.332942962646484\n",
      "Epoch 545, Loss: 40.26048278808594\n",
      "Epoch 546, Loss: 40.264739990234375\n",
      "Epoch 547, Loss: 40.44705581665039\n",
      "Epoch 548, Loss: 40.6346321105957\n",
      "Epoch 549, Loss: 40.73014831542969\n",
      "Epoch 550, Loss: 40.91435623168945\n",
      "Epoch 551, Loss: 40.51112747192383\n",
      "Epoch 552, Loss: 40.358123779296875\n",
      "Epoch 553, Loss: 40.3998908996582\n",
      "Epoch 554, Loss: 40.52853012084961\n",
      "Epoch 555, Loss: 40.539798736572266\n",
      "Epoch 556, Loss: 40.62548065185547\n",
      "Epoch 557, Loss: 40.853302001953125\n",
      "Epoch 558, Loss: 40.70317077636719\n",
      "Epoch 559, Loss: 40.62828063964844\n",
      "Epoch 560, Loss: 40.37857437133789\n",
      "Epoch 561, Loss: 40.206504821777344\n",
      "Epoch 562, Loss: 40.16249465942383\n",
      "Epoch 563, Loss: 40.18717956542969\n",
      "Epoch 564, Loss: 40.27216720581055\n",
      "Epoch 565, Loss: 40.21054458618164\n",
      "Epoch 566, Loss: 40.49419403076172\n",
      "Epoch 567, Loss: 40.53394317626953\n",
      "Epoch 568, Loss: 40.93815231323242\n",
      "Epoch 569, Loss: 40.74755096435547\n",
      "Epoch 570, Loss: 41.30915069580078\n",
      "Epoch 571, Loss: 41.1262092590332\n",
      "Epoch 572, Loss: 41.13196563720703\n",
      "Epoch 573, Loss: 41.52911376953125\n",
      "Epoch 574, Loss: 40.69045639038086\n",
      "Epoch 575, Loss: 40.784759521484375\n",
      "Epoch 576, Loss: 40.400291442871094\n",
      "Epoch 577, Loss: 40.29249572753906\n",
      "Epoch 578, Loss: 40.33335876464844\n",
      "Epoch 579, Loss: 41.3311653137207\n",
      "Epoch 580, Loss: 40.59476089477539\n",
      "Epoch 581, Loss: 40.53081512451172\n",
      "Epoch 582, Loss: 40.53955841064453\n",
      "Epoch 583, Loss: 41.20077133178711\n",
      "Epoch 584, Loss: 40.72831344604492\n",
      "Epoch 585, Loss: 40.52168273925781\n",
      "Epoch 586, Loss: 40.821449279785156\n",
      "Epoch 587, Loss: 40.86890411376953\n",
      "Epoch 588, Loss: 40.41914749145508\n",
      "Epoch 589, Loss: 40.33226776123047\n",
      "Epoch 590, Loss: 40.402740478515625\n",
      "Epoch 591, Loss: 40.917789459228516\n",
      "Epoch 592, Loss: 41.506351470947266\n",
      "Epoch 593, Loss: 41.40908432006836\n",
      "Epoch 594, Loss: 41.330345153808594\n",
      "Epoch 595, Loss: 40.762760162353516\n",
      "Epoch 596, Loss: 40.59266662597656\n",
      "Epoch 597, Loss: 40.1651611328125\n",
      "Epoch 598, Loss: 40.218257904052734\n",
      "Epoch 599, Loss: 40.13091278076172\n",
      "Epoch 600, Loss: 40.92180252075195\n",
      "Epoch 601, Loss: 40.675628662109375\n",
      "Epoch 602, Loss: 40.69590759277344\n",
      "Epoch 603, Loss: 40.72539520263672\n",
      "Epoch 604, Loss: 40.26673889160156\n",
      "Epoch 605, Loss: 40.308570861816406\n",
      "Epoch 606, Loss: 40.25958251953125\n",
      "Epoch 607, Loss: 40.340736389160156\n",
      "Epoch 608, Loss: 40.23182678222656\n",
      "Epoch 609, Loss: 40.19378662109375\n",
      "Epoch 610, Loss: 40.12757110595703\n",
      "Epoch 611, Loss: 40.15088653564453\n",
      "Epoch 612, Loss: 40.13148880004883\n",
      "Epoch 613, Loss: 40.10459899902344\n",
      "Epoch 614, Loss: 40.088775634765625\n",
      "Epoch 615, Loss: 40.129180908203125\n",
      "Epoch 616, Loss: 40.094173431396484\n",
      "Epoch 617, Loss: 40.090904235839844\n",
      "Epoch 618, Loss: 40.11478805541992\n",
      "Epoch 619, Loss: 40.1364631652832\n",
      "Epoch 620, Loss: 40.107547760009766\n",
      "Epoch 621, Loss: 40.104713439941406\n",
      "Epoch 622, Loss: 40.15373229980469\n",
      "Epoch 623, Loss: 40.13041687011719\n",
      "Epoch 624, Loss: 40.155860900878906\n",
      "Epoch 625, Loss: 40.390846252441406\n",
      "Epoch 626, Loss: 40.50505447387695\n",
      "Epoch 627, Loss: 40.43999099731445\n",
      "Epoch 628, Loss: 40.789894104003906\n",
      "Epoch 629, Loss: 40.75868225097656\n",
      "Epoch 630, Loss: 40.47106170654297\n",
      "Epoch 631, Loss: 40.758140563964844\n",
      "Epoch 632, Loss: 40.57058334350586\n",
      "Epoch 633, Loss: 40.39818572998047\n",
      "Epoch 634, Loss: 40.32734298706055\n",
      "Epoch 635, Loss: 40.129093170166016\n",
      "Epoch 636, Loss: 40.111083984375\n",
      "Epoch 637, Loss: 40.1866569519043\n",
      "Epoch 638, Loss: 40.13943099975586\n",
      "Epoch 639, Loss: 40.16274642944336\n",
      "Epoch 640, Loss: 40.11930847167969\n",
      "Epoch 641, Loss: 40.162723541259766\n",
      "Epoch 642, Loss: 40.15740966796875\n",
      "Epoch 643, Loss: 40.131431579589844\n",
      "Epoch 644, Loss: 40.143619537353516\n",
      "Epoch 645, Loss: 40.12232208251953\n",
      "Epoch 646, Loss: 40.099674224853516\n",
      "Epoch 647, Loss: 40.11121368408203\n",
      "Epoch 648, Loss: 40.13750076293945\n",
      "Epoch 649, Loss: 40.19108581542969\n",
      "Epoch 650, Loss: 40.171390533447266\n",
      "Epoch 651, Loss: 40.14594268798828\n",
      "Epoch 652, Loss: 40.10487747192383\n",
      "Epoch 653, Loss: 40.10541534423828\n",
      "Epoch 654, Loss: 40.097599029541016\n",
      "Epoch 655, Loss: 40.111175537109375\n",
      "Epoch 656, Loss: 40.12748336791992\n",
      "Epoch 657, Loss: 40.111061096191406\n",
      "Epoch 658, Loss: 40.08490753173828\n",
      "Epoch 659, Loss: 40.07916259765625\n",
      "Epoch 660, Loss: 40.07969284057617\n",
      "Epoch 661, Loss: 40.10459518432617\n",
      "Epoch 662, Loss: 40.145477294921875\n",
      "Epoch 663, Loss: 40.15898513793945\n",
      "Epoch 664, Loss: 40.26226043701172\n",
      "Epoch 665, Loss: 40.58708953857422\n",
      "Epoch 666, Loss: 40.323795318603516\n",
      "Epoch 667, Loss: 40.32781982421875\n",
      "Epoch 668, Loss: 40.506229400634766\n",
      "Epoch 669, Loss: 40.47211456298828\n",
      "Epoch 670, Loss: 40.32518768310547\n",
      "Epoch 671, Loss: 40.18600845336914\n",
      "Epoch 672, Loss: 40.10585021972656\n",
      "Epoch 673, Loss: 40.11570739746094\n",
      "Epoch 674, Loss: 40.10491180419922\n",
      "Epoch 675, Loss: 40.1566162109375\n",
      "Epoch 676, Loss: 40.108436584472656\n",
      "Epoch 677, Loss: 40.12969970703125\n",
      "Epoch 678, Loss: 40.150169372558594\n",
      "Epoch 679, Loss: 40.19019317626953\n",
      "Epoch 680, Loss: 40.1514892578125\n",
      "Epoch 681, Loss: 40.2126579284668\n",
      "Epoch 682, Loss: 40.271121978759766\n",
      "Epoch 683, Loss: 40.24109649658203\n",
      "Epoch 684, Loss: 40.13483428955078\n",
      "Epoch 685, Loss: 40.15094757080078\n",
      "Epoch 686, Loss: 40.145877838134766\n",
      "Epoch 687, Loss: 40.14620590209961\n",
      "Epoch 688, Loss: 40.11613845825195\n",
      "Epoch 689, Loss: 40.11613845825195\n",
      "Epoch 690, Loss: 40.36812210083008\n",
      "Epoch 691, Loss: 40.58319854736328\n",
      "Epoch 692, Loss: 40.43492126464844\n",
      "Epoch 693, Loss: 40.165740966796875\n",
      "Epoch 694, Loss: 40.11903762817383\n",
      "Epoch 695, Loss: 40.15886688232422\n",
      "Epoch 696, Loss: 40.173988342285156\n",
      "Epoch 697, Loss: 40.22457504272461\n",
      "Epoch 698, Loss: 40.167991638183594\n",
      "Epoch 699, Loss: 40.1063346862793\n",
      "Epoch 700, Loss: 40.07216262817383\n",
      "Epoch 701, Loss: 40.080299377441406\n",
      "Epoch 702, Loss: 40.1092529296875\n",
      "Epoch 703, Loss: 40.09687423706055\n",
      "Epoch 704, Loss: 40.131492614746094\n",
      "Epoch 705, Loss: 40.24766540527344\n",
      "Epoch 706, Loss: 40.44587707519531\n",
      "Epoch 707, Loss: 40.5352783203125\n",
      "Epoch 708, Loss: 40.40854263305664\n",
      "Epoch 709, Loss: 40.35312271118164\n",
      "Epoch 710, Loss: 40.17395782470703\n",
      "Epoch 711, Loss: 40.127777099609375\n",
      "Epoch 712, Loss: 40.145660400390625\n",
      "Epoch 713, Loss: 40.1454963684082\n",
      "Epoch 714, Loss: 40.09748077392578\n",
      "Epoch 715, Loss: 40.132347106933594\n",
      "Epoch 716, Loss: 40.11455154418945\n",
      "Epoch 717, Loss: 40.14239501953125\n",
      "Epoch 718, Loss: 40.10879135131836\n",
      "Epoch 719, Loss: 40.09149169921875\n",
      "Epoch 720, Loss: 40.14661407470703\n",
      "Epoch 721, Loss: 40.138824462890625\n",
      "Epoch 722, Loss: 40.12397384643555\n",
      "Epoch 723, Loss: 40.150455474853516\n",
      "Epoch 724, Loss: 40.2070426940918\n",
      "Epoch 725, Loss: 40.23261642456055\n",
      "Epoch 726, Loss: 40.17353057861328\n",
      "Epoch 727, Loss: 40.13178634643555\n",
      "Epoch 728, Loss: 40.10688018798828\n",
      "Epoch 729, Loss: 40.104488372802734\n",
      "Epoch 730, Loss: 40.10808181762695\n",
      "Epoch 731, Loss: 40.084808349609375\n",
      "Epoch 732, Loss: 40.07670593261719\n",
      "Epoch 733, Loss: 40.08407974243164\n",
      "Epoch 734, Loss: 40.086456298828125\n",
      "Epoch 735, Loss: 40.097190856933594\n",
      "Epoch 736, Loss: 40.11551284790039\n",
      "Epoch 737, Loss: 40.11936950683594\n",
      "Epoch 738, Loss: 40.101043701171875\n",
      "Epoch 739, Loss: 40.16394805908203\n",
      "Epoch 740, Loss: 40.14325714111328\n",
      "Epoch 741, Loss: 40.152645111083984\n",
      "Epoch 742, Loss: 40.12919998168945\n",
      "Epoch 743, Loss: 40.07958984375\n",
      "Epoch 744, Loss: 40.0776481628418\n",
      "Epoch 745, Loss: 40.108367919921875\n",
      "Epoch 746, Loss: 40.10594940185547\n",
      "Epoch 747, Loss: 40.10552215576172\n",
      "Epoch 748, Loss: 40.10164260864258\n",
      "Epoch 749, Loss: 40.101993560791016\n",
      "Epoch 750, Loss: 40.10565185546875\n",
      "Epoch 751, Loss: 40.10950469970703\n",
      "Epoch 752, Loss: 40.1149787902832\n",
      "Epoch 753, Loss: 40.11865997314453\n",
      "Epoch 754, Loss: 40.14239501953125\n",
      "Epoch 755, Loss: 40.1275749206543\n",
      "Epoch 756, Loss: 40.12462615966797\n",
      "Epoch 757, Loss: 40.10055160522461\n",
      "Epoch 758, Loss: 40.08049392700195\n",
      "Epoch 759, Loss: 40.07879638671875\n",
      "Epoch 760, Loss: 40.094200134277344\n",
      "Epoch 761, Loss: 40.11116409301758\n",
      "Epoch 762, Loss: 40.14866638183594\n",
      "Epoch 763, Loss: 40.23638916015625\n",
      "Epoch 764, Loss: 40.140625\n",
      "Epoch 765, Loss: 40.093955993652344\n",
      "Epoch 766, Loss: 40.09415817260742\n",
      "Epoch 767, Loss: 40.10207748413086\n",
      "Epoch 768, Loss: 40.142120361328125\n",
      "Epoch 769, Loss: 40.145263671875\n",
      "Epoch 770, Loss: 40.1120491027832\n",
      "Epoch 771, Loss: 40.148040771484375\n",
      "Epoch 772, Loss: 40.108699798583984\n",
      "Epoch 773, Loss: 40.115440368652344\n",
      "Epoch 774, Loss: 40.141815185546875\n",
      "Epoch 775, Loss: 40.160675048828125\n",
      "Epoch 776, Loss: 40.103946685791016\n",
      "Epoch 777, Loss: 40.10369873046875\n",
      "Epoch 778, Loss: 40.09200668334961\n",
      "Epoch 779, Loss: 40.10272216796875\n",
      "Epoch 780, Loss: 40.085262298583984\n",
      "Epoch 781, Loss: 40.094017028808594\n",
      "Epoch 782, Loss: 40.09482192993164\n",
      "Epoch 783, Loss: 40.061180114746094\n",
      "Epoch 784, Loss: 40.0690803527832\n",
      "Epoch 785, Loss: 40.06886672973633\n",
      "Epoch 786, Loss: 40.066261291503906\n",
      "Epoch 787, Loss: 40.06293487548828\n",
      "Epoch 788, Loss: 40.054779052734375\n",
      "Epoch 789, Loss: 40.065032958984375\n",
      "Epoch 790, Loss: 40.086647033691406\n",
      "Epoch 791, Loss: 40.068233489990234\n",
      "Epoch 792, Loss: 40.06973648071289\n",
      "Epoch 793, Loss: 40.07571029663086\n",
      "Epoch 794, Loss: 40.09188461303711\n",
      "Epoch 795, Loss: 40.07653045654297\n",
      "Epoch 796, Loss: 40.06636428833008\n",
      "Epoch 797, Loss: 40.087345123291016\n",
      "Epoch 798, Loss: 40.08852767944336\n",
      "Epoch 799, Loss: 40.09561538696289\n",
      "Epoch 800, Loss: 40.09510803222656\n",
      "Epoch 801, Loss: 40.06130599975586\n",
      "Epoch 802, Loss: 40.054710388183594\n",
      "Epoch 803, Loss: 40.05070495605469\n",
      "Epoch 804, Loss: 40.05121994018555\n",
      "Epoch 805, Loss: 40.049278259277344\n",
      "Epoch 806, Loss: 40.05001449584961\n",
      "Epoch 807, Loss: 40.04789733886719\n",
      "Epoch 808, Loss: 40.05808639526367\n",
      "Epoch 809, Loss: 40.048858642578125\n",
      "Epoch 810, Loss: 40.052974700927734\n",
      "Epoch 811, Loss: 40.04875946044922\n",
      "Epoch 812, Loss: 40.05891036987305\n",
      "Epoch 813, Loss: 40.04655456542969\n",
      "Epoch 814, Loss: 40.04808044433594\n",
      "Epoch 815, Loss: 40.04999542236328\n",
      "Epoch 816, Loss: 40.05948257446289\n",
      "Epoch 817, Loss: 40.051475524902344\n",
      "Epoch 818, Loss: 40.05669021606445\n",
      "Epoch 819, Loss: 40.06690979003906\n",
      "Epoch 820, Loss: 40.07359313964844\n",
      "Epoch 821, Loss: 40.05575942993164\n",
      "Epoch 822, Loss: 40.06100845336914\n",
      "Epoch 823, Loss: 40.055137634277344\n",
      "Epoch 824, Loss: 40.05169677734375\n",
      "Epoch 825, Loss: 40.05082702636719\n",
      "Epoch 826, Loss: 40.04835891723633\n",
      "Epoch 827, Loss: 40.049068450927734\n",
      "Epoch 828, Loss: 40.052696228027344\n",
      "Epoch 829, Loss: 40.045597076416016\n",
      "Epoch 830, Loss: 40.051727294921875\n",
      "Epoch 831, Loss: 40.04505157470703\n",
      "Epoch 832, Loss: 40.04630661010742\n",
      "Epoch 833, Loss: 40.051429748535156\n",
      "Epoch 834, Loss: 40.04883575439453\n",
      "Epoch 835, Loss: 40.050235748291016\n",
      "Epoch 836, Loss: 40.04744338989258\n",
      "Epoch 837, Loss: 40.05784606933594\n",
      "Epoch 838, Loss: 40.04645538330078\n",
      "Epoch 839, Loss: 40.050453186035156\n",
      "Epoch 840, Loss: 40.049617767333984\n",
      "Epoch 841, Loss: 40.04885482788086\n",
      "Epoch 842, Loss: 40.04924011230469\n",
      "Epoch 843, Loss: 40.0543327331543\n",
      "Epoch 844, Loss: 40.04823303222656\n",
      "Epoch 845, Loss: 40.04973220825195\n",
      "Epoch 846, Loss: 40.05276870727539\n",
      "Epoch 847, Loss: 40.049156188964844\n",
      "Epoch 848, Loss: 40.0480842590332\n",
      "Epoch 849, Loss: 40.054080963134766\n",
      "Epoch 850, Loss: 40.045814514160156\n",
      "Epoch 851, Loss: 40.048397064208984\n",
      "Epoch 852, Loss: 40.05128860473633\n",
      "Epoch 853, Loss: 40.049217224121094\n",
      "Epoch 854, Loss: 40.052886962890625\n",
      "Epoch 855, Loss: 40.049617767333984\n",
      "Epoch 856, Loss: 40.05031204223633\n",
      "Epoch 857, Loss: 40.04920196533203\n",
      "Epoch 858, Loss: 40.05352020263672\n",
      "Epoch 859, Loss: 40.0438232421875\n",
      "Epoch 860, Loss: 40.048744201660156\n",
      "Epoch 861, Loss: 40.05017852783203\n",
      "Epoch 862, Loss: 40.05268859863281\n",
      "Epoch 863, Loss: 40.05316925048828\n",
      "Epoch 864, Loss: 40.04962158203125\n",
      "Epoch 865, Loss: 40.05708694458008\n",
      "Epoch 866, Loss: 40.054229736328125\n",
      "Epoch 867, Loss: 40.05284881591797\n",
      "Epoch 868, Loss: 40.051944732666016\n",
      "Epoch 869, Loss: 40.050357818603516\n",
      "Epoch 870, Loss: 40.057315826416016\n",
      "Epoch 871, Loss: 40.05231475830078\n",
      "Epoch 872, Loss: 40.05320739746094\n",
      "Epoch 873, Loss: 40.05158996582031\n",
      "Epoch 874, Loss: 40.0488395690918\n",
      "Epoch 875, Loss: 40.054264068603516\n",
      "Epoch 876, Loss: 40.05519485473633\n",
      "Epoch 877, Loss: 40.05110168457031\n",
      "Epoch 878, Loss: 40.053016662597656\n",
      "Epoch 879, Loss: 40.05689239501953\n",
      "Epoch 880, Loss: 40.05721664428711\n",
      "Epoch 881, Loss: 40.05351638793945\n",
      "Epoch 882, Loss: 40.05094528198242\n",
      "Epoch 883, Loss: 40.05965042114258\n",
      "Epoch 884, Loss: 40.04862594604492\n",
      "Epoch 885, Loss: 40.057769775390625\n",
      "Epoch 886, Loss: 40.053810119628906\n",
      "Epoch 887, Loss: 40.049835205078125\n",
      "Epoch 888, Loss: 40.0528450012207\n",
      "Epoch 889, Loss: 40.053348541259766\n",
      "Epoch 890, Loss: 40.04994201660156\n",
      "Epoch 891, Loss: 40.05267333984375\n",
      "Epoch 892, Loss: 40.05186080932617\n",
      "Epoch 893, Loss: 40.05361557006836\n",
      "Epoch 894, Loss: 40.054439544677734\n",
      "Epoch 895, Loss: 40.05704879760742\n",
      "Epoch 896, Loss: 40.0538444519043\n",
      "Epoch 897, Loss: 40.05568313598633\n",
      "Epoch 898, Loss: 40.053855895996094\n",
      "Epoch 899, Loss: 40.06357955932617\n",
      "Epoch 900, Loss: 40.054988861083984\n",
      "Epoch 901, Loss: 40.05669403076172\n",
      "Epoch 902, Loss: 40.06063461303711\n",
      "Epoch 903, Loss: 40.055397033691406\n",
      "Epoch 904, Loss: 40.05431365966797\n",
      "Epoch 905, Loss: 40.058738708496094\n",
      "Epoch 906, Loss: 40.058536529541016\n",
      "Epoch 907, Loss: 40.05842971801758\n",
      "Epoch 908, Loss: 40.05731201171875\n",
      "Epoch 909, Loss: 40.05487060546875\n",
      "Epoch 910, Loss: 40.059837341308594\n",
      "Epoch 911, Loss: 40.04876708984375\n",
      "Epoch 912, Loss: 40.04259490966797\n",
      "Epoch 913, Loss: 40.04165267944336\n",
      "Epoch 914, Loss: 40.0412483215332\n",
      "Epoch 915, Loss: 40.04135513305664\n",
      "Epoch 916, Loss: 40.041168212890625\n",
      "Epoch 917, Loss: 40.04113006591797\n",
      "Epoch 918, Loss: 40.041019439697266\n",
      "Epoch 919, Loss: 40.04096221923828\n",
      "Epoch 920, Loss: 40.04087448120117\n",
      "Epoch 921, Loss: 40.040863037109375\n",
      "Epoch 922, Loss: 40.040771484375\n",
      "Epoch 923, Loss: 40.040687561035156\n",
      "Epoch 924, Loss: 40.0406608581543\n",
      "Epoch 925, Loss: 40.04058837890625\n",
      "Epoch 926, Loss: 40.04055404663086\n",
      "Epoch 927, Loss: 40.04047393798828\n",
      "Epoch 928, Loss: 40.04045486450195\n",
      "Epoch 929, Loss: 40.040374755859375\n",
      "Epoch 930, Loss: 40.04029083251953\n",
      "Epoch 931, Loss: 40.04025650024414\n",
      "Epoch 932, Loss: 40.040218353271484\n",
      "Epoch 933, Loss: 40.040130615234375\n",
      "Epoch 934, Loss: 40.04008483886719\n",
      "Epoch 935, Loss: 40.0400505065918\n",
      "Epoch 936, Loss: 40.03998947143555\n",
      "Epoch 937, Loss: 40.03995132446289\n",
      "Epoch 938, Loss: 40.03992462158203\n",
      "Epoch 939, Loss: 40.03984069824219\n",
      "Epoch 940, Loss: 40.03982925415039\n",
      "Epoch 941, Loss: 40.03973388671875\n",
      "Epoch 942, Loss: 40.03974151611328\n",
      "Epoch 943, Loss: 40.039669036865234\n",
      "Epoch 944, Loss: 40.039634704589844\n",
      "Epoch 945, Loss: 40.039546966552734\n",
      "Epoch 946, Loss: 40.03954315185547\n",
      "Epoch 947, Loss: 40.03950119018555\n",
      "Epoch 948, Loss: 40.03947067260742\n",
      "Epoch 949, Loss: 40.03941345214844\n",
      "Epoch 950, Loss: 40.03939437866211\n",
      "Epoch 951, Loss: 40.03936004638672\n",
      "Epoch 952, Loss: 40.03929138183594\n",
      "Epoch 953, Loss: 40.03929901123047\n",
      "Epoch 954, Loss: 40.03925704956055\n",
      "Epoch 955, Loss: 40.03919219970703\n",
      "Epoch 956, Loss: 40.039154052734375\n",
      "Epoch 957, Loss: 40.039146423339844\n",
      "Epoch 958, Loss: 40.039119720458984\n",
      "Epoch 959, Loss: 40.0390739440918\n",
      "Epoch 960, Loss: 40.03900146484375\n",
      "Epoch 961, Loss: 40.039031982421875\n",
      "Epoch 962, Loss: 40.038978576660156\n",
      "Epoch 963, Loss: 40.03895568847656\n",
      "Epoch 964, Loss: 40.038909912109375\n",
      "Epoch 965, Loss: 40.03891372680664\n",
      "Epoch 966, Loss: 40.038902282714844\n",
      "Epoch 967, Loss: 40.03883743286133\n",
      "Epoch 968, Loss: 40.038841247558594\n",
      "Epoch 969, Loss: 40.038818359375\n",
      "Epoch 970, Loss: 40.03877258300781\n",
      "Epoch 971, Loss: 40.03876495361328\n",
      "Epoch 972, Loss: 40.03873825073242\n",
      "Epoch 973, Loss: 40.03871536254883\n",
      "Epoch 974, Loss: 40.03867721557617\n",
      "Epoch 975, Loss: 40.038673400878906\n",
      "Epoch 976, Loss: 40.03865051269531\n",
      "Epoch 977, Loss: 40.03860092163086\n",
      "Epoch 978, Loss: 40.03858947753906\n",
      "Epoch 979, Loss: 40.03862762451172\n",
      "Epoch 980, Loss: 40.03864288330078\n",
      "Epoch 981, Loss: 40.03857421875\n",
      "Epoch 982, Loss: 40.03856658935547\n",
      "Epoch 983, Loss: 40.038543701171875\n",
      "Epoch 984, Loss: 40.03852844238281\n",
      "Epoch 985, Loss: 40.038551330566406\n",
      "Epoch 986, Loss: 40.038516998291016\n",
      "Epoch 987, Loss: 40.03848648071289\n",
      "Epoch 988, Loss: 40.038455963134766\n",
      "Epoch 989, Loss: 40.03845977783203\n",
      "Epoch 990, Loss: 40.03842544555664\n",
      "Epoch 991, Loss: 40.03843307495117\n",
      "Epoch 992, Loss: 40.038509368896484\n",
      "Epoch 993, Loss: 40.03847122192383\n",
      "Epoch 994, Loss: 40.038429260253906\n",
      "Epoch 995, Loss: 40.03843688964844\n",
      "Epoch 996, Loss: 40.03848648071289\n",
      "Epoch 997, Loss: 40.038475036621094\n",
      "Epoch 998, Loss: 40.0384635925293\n",
      "Epoch 999, Loss: 40.038475036621094\n",
      "Epoch 1000, Loss: 40.03842544555664\n"
     ]
    }
   ],
   "source": [
    "## Training\n",
    "import os\n",
    "\n",
    "# Directory to save model parameters\n",
    "save_dir = \"saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Lists to store training loss and model parameters\n",
    "training_losses = []\n",
    "model_params_list = []\n",
    "\n",
    "num_epochs = 1000\n",
    "num_batches = 16\n",
    "batch_size = embedded_sequences.shape[0] // num_batches\n",
    "\n",
    "# Define optimizer with cosine decay schedule\n",
    "num_train_steps = num_epochs * num_batches\n",
    "lr_schedule = optax.cosine_decay_schedule(1.0, num_train_steps)\n",
    "optimizer = optax.chain(optax.adam(learning_rate=lr_schedule), optax.clip_by_global_norm(1.0))\n",
    "\n",
    "# Initialize model and optimizer state\n",
    "rng = random.PRNGKey(0)\n",
    "params = model.init(rng, jnp.zeros((1, T, d_2), dtype=jnp.int32))\n",
    "\n",
    "optimizer_state = optimizer.init(params)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        # Get batch\n",
    "        batch_sequences = embedded_sequences[batch_idx * batch_size : (batch_idx + 1) * batch_size] #embedded \n",
    "        batch_targets = sequences_onehot[batch_idx * batch_size + 1 : (batch_idx + 1) * batch_size + 1] #not embeddeds\n",
    "        \n",
    "        # Compute gradients and loss\n",
    "        def loss_fn(params):\n",
    "            logits = model.apply(params, batch_sequences)\n",
    "            loss = cross_entropy_loss(logits, batch_targets)\n",
    "            return loss, logits\n",
    "        \n",
    "        grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "        (loss, _), grads = grad_fn(params)\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        # Update parameters\n",
    "        updates, optimizer_state = optimizer.update(grads, optimizer_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "    \n",
    "    # Calculate average epoch loss\n",
    "    avg_epoch_loss = epoch_loss / num_batches\n",
    "    training_losses.append(avg_epoch_loss)\n",
    "\n",
    "    # Save model parameters\n",
    "    if (epoch + 1) % 25 == 0:\n",
    "        model_params_list.append(params)\n",
    "        model_path = os.path.join(save_dir, f\"model_epoch_{epoch+1}.params\")\n",
    "        with open(model_path, \"wb\") as f:\n",
    "            f.write(flax.serialization.to_bytes(params))\n",
    "    \n",
    "    # Print epoch loss\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_epoch_loss}\")\n",
    "\n",
    "# Save training losses to file\n",
    "losses_path = os.path.join(save_dir, \"training_losses.npy\")\n",
    "np.save(losses_path, np.array(training_losses))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

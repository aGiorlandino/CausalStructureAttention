{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-layer architecture for next token prediction task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports and Utility functions\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import numpy as np\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "import flax.serialization\n",
    "\n",
    "# Define utility functions for attention mechanism\n",
    "def causal_mask(size):\n",
    "    mask = np.tril(np.ones((size, size), dtype=np.bool_), k=0)\n",
    "    return jnp.array(mask)\n",
    "\n",
    "def cross_entropy_loss(logits, labels):\n",
    "    log_probs = jax.nn.log_softmax(logits)\n",
    "    return -jnp.sum(labels * log_probs) / labels.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    embed_dim: int\n",
    "    num_heads: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.head_dim = self.embed_dim \n",
    "        self.qkv = nn.Dense(features=self.embed_dim * 3 * self.num_heads, use_bias=False)\n",
    "        self.out = nn.Dense(features=self.embed_dim)\n",
    "\n",
    "    def __call__(self, x, mask=None):\n",
    "        batch_size, seq_length,_ = x.shape\n",
    "        qkv = self.qkv(x) #qkb are not the params, theyre W_Q, W_K, W_V applied to the batch !\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3, self.head_dim)\n",
    "        qkv = qkv.transpose((2, 0, 1, 3, 4))  # (num_heads, batch_size, seq_length, 3, head_dim)\n",
    "        q, k, v = qkv[:, :, :, 0, :], qkv[:, :, :, 1, :], qkv[:, :, :, 2, :]\n",
    "        attn_weights = jnp.einsum('hbqd,hbkd->hbqk', q, k) / jnp.sqrt(self.head_dim) # einstein summation\n",
    "        #then check if we want to normalize or not / jnp.sqrt(self.head_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            attn_weights = jnp.where(mask[None, None, :, :], attn_weights, -1e10)\n",
    "\n",
    "        attn_weights = jax.nn.softmax(attn_weights, axis=-1) #axis=-1 is the last axis i.e. 'row-wise'\n",
    "        attn_output = jnp.einsum('hbqk,hbvd->hbqd', attn_weights, v) # (num_heads, batch_size, seq_length, head_dim)\n",
    "        attn_output = attn_output.transpose((1, 2, 0, 3))  # (batch_size, seq_length, num_heads, head_dim)\n",
    "        attn_output = attn_output.reshape(batch_size, seq_length, self.num_heads * self.head_dim)\n",
    "        return self.out(attn_output)\n",
    "\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    embed_dim: int\n",
    "    num_heads: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.self_attn = MultiHeadSelfAttention(embed_dim=self.embed_dim, num_heads=self.num_heads)\n",
    "        self.ln = nn.LayerNorm()\n",
    "\n",
    "    def __call__(self, x, mask=None):\n",
    "        attn_output = self.self_attn(x, mask=mask)\n",
    "        x = x + attn_output\n",
    "        x = self.ln(x)\n",
    "        return x\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    #vocab_size: int\n",
    "    layer_dims: list\n",
    "    num_heads: list\n",
    "\n",
    "    def setup(self):\n",
    "        #self.embedding = nn.Embed(num_embeddings=self.vocab_size, features=self.layer_dims[0]) # no embedding!\n",
    "        self.layers = [TransformerDecoderLayer(embed_dim=layer_dim, num_heads=num_heads) \n",
    "                       for layer_dim, num_heads in zip(self.layer_dims, self.num_heads)]\n",
    "        self.ln = nn.LayerNorm()\n",
    "\n",
    "    def __call__(self, x, mask=None):\n",
    "        #x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=mask)\n",
    "        x = self.ln(x)\n",
    "        return x\n",
    "\n",
    "class NextTokenPredictor(nn.Module):\n",
    "    vocab_size: int\n",
    "    layer_dims: list\n",
    "    num_heads: list\n",
    "\n",
    "    def setup(self):\n",
    "        self.decoder = TransformerDecoder(\n",
    "            #vocab_size=self.vocab_size,\n",
    "            layer_dims=self.layer_dims,\n",
    "            num_heads=self.num_heads\n",
    "        )\n",
    "        self.out = nn.Dense(features=self.vocab_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        seq_length = x.shape[1]\n",
    "        mask = causal_mask(seq_length)\n",
    "        decoder_output = self.decoder(x, mask=mask)\n",
    "        logits = self.out(decoder_output)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model definition\n",
    "\n",
    "S = 5  # Cardinality of the alphabet\n",
    "T = 25  # Sequence length\n",
    "m1 = 2  # Heads in the first layer\n",
    "m2 = 1  # Heads in the second layer\n",
    "num_heads = [m1, m2]  # Number of heads for each layer\n",
    "\n",
    "d_0 = S + T # Dimension of the input sequence\n",
    "d_1 = (1 + m1) * d_0  \n",
    "d_2 = (1 + m2) * d_1  # embedding dimension  \n",
    "# definition of d_l in the paper. \n",
    "\n",
    "\n",
    "\n",
    "vocab_size = S\n",
    "layer_dims = [d_2, d_2]  \n",
    "\n",
    "model = NextTokenPredictor(\n",
    "    vocab_size=vocab_size,\n",
    "    layer_dims=layer_dims,\n",
    "    num_heads=num_heads\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of loaded sequences: (25000, 25)\n",
      "Shape of embedded sequences: (25000, 25, 180)\n"
     ]
    }
   ],
   "source": [
    "# Data Generation \n",
    "    \n",
    "def create_3gram_transition_matrix(S):\n",
    "    \"\"\"\n",
    "    Create a transition matrix for a 3-gram model by sampling from a Dirichlet prior.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are 2-grams (tuples of integers) and values are the probability distributions\n",
    "          over the next words (integers from 0 to S-1).\n",
    "    \"\"\"\n",
    "    # Create a list of all possible 2-grams from the vocabulary\n",
    "    two_grams = [(i, j) for i in range(S) for j in range(S)]\n",
    "\n",
    "    transition_matrix = {}     # Initialize the transition matrix as a dictionary\n",
    "    alpha = 1.0     # Dirichlet parameter alpha\n",
    " \n",
    "    for two_gram in two_grams:\n",
    "        next_word_probs = np.random.dirichlet([alpha] * S)         # Sample a probability distribution over the next words\n",
    "        transition_matrix[two_gram] = next_word_probs\n",
    "    return transition_matrix\n",
    "\n",
    "\n",
    "def generate_sequence(transition_matrix, T):\n",
    "    \"\"\"\n",
    "    Generate a sequence of length T given a transition matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    transition_matrix (dict): The transition matrix where keys are 2-grams (tuples of integers)\n",
    "                              and values are the probability distributions over the next words.\n",
    "    T (int): Length of the sequence to generate.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of integers representing the generated sequence.\n",
    "    \"\"\"\n",
    "    two_grams = list(transition_matrix.keys())     # Extract the list of 2-grams from the transition matrix\n",
    "    \n",
    "    # Randomly choose an initial 2-gram\n",
    "    current_2gram = two_grams[np.random.choice(len(two_grams))]\n",
    "    \n",
    "    sequence = list(current_2gram)     # Initialize the sequence with the chosen 2-gram\n",
    "    \n",
    "    # Generate the sequence\n",
    "    for _ in range(T - 2):\n",
    "        next_word_probs = transition_matrix[current_2gram]      # Get the next word probability distribution for the current 2-gram\n",
    "        next_word = np.random.choice(range(len(next_word_probs)), p=next_word_probs)         # Sample the next word \n",
    "        sequence.append(next_word)\n",
    "        # Update the current 2-gram\n",
    "        current_2gram = (current_2gram[1], next_word)\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding \n",
    "def embed(sequences):\n",
    "    embedded_sequences = np.zeros((sequences.shape[0], sequences.shape[1], d_2), dtype=np.int32)\n",
    "    for i in range(sequences.shape[0]):\n",
    "        for j in range(sequences.shape[1]):\n",
    "            embedded_sequences[i, j, sequences[i, j]] = 1\n",
    "            embedded_sequences[i, j, S + j] = 1\n",
    "    return embedded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 51.807762145996094\n",
      "Epoch 2, Loss: 41.703086853027344\n",
      "Epoch 3, Loss: 40.71582794189453\n",
      "Epoch 4, Loss: 40.68227767944336\n",
      "Epoch 5, Loss: 44.09806823730469\n",
      "Epoch 6, Loss: 41.85529327392578\n",
      "Epoch 7, Loss: 42.92106628417969\n",
      "Epoch 8, Loss: 42.15922546386719\n",
      "Epoch 9, Loss: 42.38582229614258\n"
     ]
    }
   ],
   "source": [
    "## Training\n",
    "import os\n",
    "\n",
    "# Directory to save model parameters\n",
    "save_dir = \"saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Lists to store training loss and model parameters\n",
    "training_losses = []\n",
    "model_params_list = []\n",
    "\n",
    "num_epochs = 2048\n",
    "num_batches = 64\n",
    "batch_size = 1024\n",
    "\n",
    "# Define optimizer with cosine decay schedule\n",
    "num_train_steps = num_epochs * num_batches\n",
    "lr_schedule = optax.cosine_decay_schedule(0.3, num_train_steps)\n",
    "optimizer = optax.chain(optax.adam(learning_rate=lr_schedule), optax.clip_by_global_norm(1.0))\n",
    "\n",
    "# Initialize model and optimizer state\n",
    "rng = random.PRNGKey(0)\n",
    "params = model.init(rng, jnp.zeros((1, T, d_2), dtype=jnp.int32))\n",
    "\n",
    "optimizer_state = optimizer.init(params)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    transition_matrix = create_3gram_transition_matrix(S) # 3-gram transition fixed for the epoch\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        sequences = np.array([generate_sequence(transition_matrix, T) for _ in range(batch_size)])\n",
    "\n",
    "\n",
    "      # Generate target sequences by shifting the input sequences\n",
    "        target_sequences = np.zeros_like(sequences)\n",
    "        target_sequences[:, :-1] = sequences[:, 1:]\n",
    "        \n",
    "        # Convert target sequences to one-hot encoding\n",
    "        sequences_onehot = np.zeros((sequences.shape[0], sequences.shape[1], S), dtype=np.int32)\n",
    "        target_sequences_onehot = np.zeros((target_sequences.shape[0], target_sequences.shape[1], S), dtype=np.int32)\n",
    "        for i in range(sequences.shape[0]):\n",
    "            for j in range(sequences.shape[1]):\n",
    "                sequences_onehot[i, j, sequences[i, j]] = 1\n",
    "                target_sequences_onehot[i, j, target_sequences[i, j]] = 1\n",
    "        \n",
    "        # Embed input sequences\n",
    "        embedded_sequences = embed(sequences)\n",
    "        \n",
    "        # Convert embedded sequences and targets to JAX arrays\n",
    "        batch_sequences = jnp.array(embedded_sequences)\n",
    "        batch_targets = jnp.array(target_sequences_onehot)\n",
    "\n",
    "        \n",
    "        # Compute gradients and loss\n",
    "        def loss_fn(params):\n",
    "            logits = model.apply(params, batch_sequences)\n",
    "            loss = cross_entropy_loss(logits, batch_targets)\n",
    "            return loss, logits\n",
    "        \n",
    "        grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "        (loss, _), grads = grad_fn(params)\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        # Update parameters\n",
    "        updates, optimizer_state = optimizer.update(grads, optimizer_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "    \n",
    "    # Calculate average epoch loss\n",
    "    avg_epoch_loss = epoch_loss / num_batches\n",
    "    training_losses.append(avg_epoch_loss)\n",
    "\n",
    "    # Save model parameters\n",
    "    if (epoch + 1) % 25 == 0:\n",
    "        model_params_list.append(params)\n",
    "        model_path = os.path.join(save_dir, f\"model_epoch_{epoch+1}.params\")\n",
    "        with open(model_path, \"wb\") as f:\n",
    "            f.write(flax.serialization.to_bytes(params))\n",
    "    \n",
    "    # Print epoch loss\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_epoch_loss}\")\n",
    "\n",
    "# Save training losses to file\n",
    "losses_path = os.path.join(save_dir, \"training_losses.npy\")\n",
    "np.save(losses_path, np.array(training_losses))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
